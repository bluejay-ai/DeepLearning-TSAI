{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st DNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluejay-ai/DeepLearning-TSAI/blob/master/Project4-ArchitecturalBasics/1st_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "7604f20e-ccec-4276-b99b-c9eea71d8657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f48a1b47d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "9e92e515-a0fa-425e-fc6e-394318b7b812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "a94e8508-f883-4609-ef39-073bf0770d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyLn07MYphsz",
        "colab_type": "text"
      },
      "source": [
        "**FIRST CODE**\n",
        "\n",
        "This is my first vanilla code. Here, I am defining my basic architecture. The goal is to achieve \n",
        "- 99.4% validation accuracy using less than 15K parameters. \n",
        "\n",
        "The concepts applied here are as following :\n",
        "1. Position of max pooling layers\n",
        "2. Number of Convolution layers\n",
        "3. Number of Kernels\n",
        "4. Achiving Global Receiptive Field equal to size of image\n",
        "5. ReLu activation \n",
        "\n",
        "The First Model acheived validation accuracy of *99.01*% with 14,776 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "1259049c-f7ac-404f-82ad-18261a8db144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model.add(Convolution2D(10, 3, 3,activation='relu')) #24\n",
        "model.add(Convolution2D(20, 3, 3,activation='relu')) #22\n",
        "model.add(Convolution2D(10, 3, 3,activation='relu')) #20\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #10\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3,activation='relu')) #8\n",
        "\n",
        "model.add(Convolution2D(10, 3, 3,activation='relu')) #6\n",
        "model.add(Convolution2D(20, 3, 3,activation='relu')) #4\n",
        "\n",
        "model.add(Convolution2D(10, 4,4)) #1\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, verbose=1)\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_43\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_303 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_304 (Conv2D)          (None, 24, 24, 10)        2890      \n",
            "_________________________________________________________________\n",
            "conv2d_305 (Conv2D)          (None, 22, 22, 20)        1820      \n",
            "_________________________________________________________________\n",
            "conv2d_306 (Conv2D)          (None, 20, 20, 10)        1810      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_46 (MaxPooling (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_307 (Conv2D)          (None, 8, 8, 16)          1456      \n",
            "_________________________________________________________________\n",
            "conv2d_308 (Conv2D)          (None, 6, 6, 10)          1450      \n",
            "_________________________________________________________________\n",
            "conv2d_309 (Conv2D)          (None, 4, 4, 20)          1820      \n",
            "_________________________________________________________________\n",
            "conv2d_310 (Conv2D)          (None, 1, 1, 10)          3210      \n",
            "_________________________________________________________________\n",
            "flatten_43 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,776\n",
            "Trainable params: 14,776\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.2208 - acc: 0.9319\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0713 - acc: 0.9777\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0556 - acc: 0.9827\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 13s 212us/step - loss: 0.0464 - acc: 0.9853\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0396 - acc: 0.9876\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0352 - acc: 0.9888\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0309 - acc: 0.9904\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0279 - acc: 0.9912\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0262 - acc: 0.9917\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0233 - acc: 0.9927\n",
            "[0.03576316494356288, 0.9901]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "5003795b-b479-44f6-93e3-b7f5b567fd63"
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.25086086e-09 1.24469923e-09 3.90403407e-08 1.72716128e-07\n",
            "  1.26675640e-11 1.17200849e-09 4.73682085e-15 9.99999762e-01\n",
            "  8.83600859e-10 1.84489526e-08]\n",
            " [3.86065011e-08 5.31040878e-10 1.00000000e+00 2.58421539e-13\n",
            "  2.33468084e-12 4.02942762e-13 3.30003544e-08 2.06277431e-16\n",
            "  1.96419381e-10 1.43492294e-14]\n",
            " [2.39785902e-09 9.99998569e-01 2.64920974e-10 4.97257275e-08\n",
            "  4.37571174e-07 5.92102140e-07 2.63120086e-07 1.13524983e-07\n",
            "  5.81139794e-08 4.93869112e-10]\n",
            " [9.99980330e-01 1.30831425e-18 3.46756490e-09 1.49709745e-10\n",
            "  1.10153907e-10 2.05025885e-10 1.96955862e-05 1.41442760e-11\n",
            "  2.69789613e-09 2.43892240e-09]\n",
            " [8.05117306e-09 1.02115321e-11 8.25879621e-13 3.94001368e-12\n",
            "  9.99995470e-01 9.37630529e-10 2.04139701e-06 4.02260582e-13\n",
            "  1.30786668e-06 1.22754614e-06]\n",
            " [2.56451327e-09 9.99997020e-01 2.52607379e-09 2.26379683e-07\n",
            "  1.73683009e-06 2.22310433e-07 7.26312166e-08 3.34581870e-07\n",
            "  3.23251300e-07 3.75580766e-09]\n",
            " [1.98603848e-17 1.06836907e-10 5.04858437e-14 1.75965640e-14\n",
            "  9.99997258e-01 4.08177758e-09 3.21542619e-12 3.92177046e-09\n",
            "  2.84993689e-07 2.42970896e-06]\n",
            " [7.79300832e-13 7.89844264e-14 2.23376442e-10 1.54804711e-11\n",
            "  4.05210885e-05 9.00713254e-11 1.01311675e-17 4.14612476e-11\n",
            "  2.54778847e-08 9.99959469e-01]\n",
            " [1.16387044e-09 1.06380519e-12 3.14902218e-06 2.45918924e-10\n",
            "  4.91448687e-11 9.98966098e-01 3.89727575e-05 1.25273725e-09\n",
            "  9.91518609e-04 2.93827100e-07]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "outputId": "6a73b78d-c614-4d4a-c280-71869f0af048"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_307'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwYAAALUCAYAAACxRyVEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdebRtV10n+u8smmAIkABJSAMJTYIE\nEUQEkVZAOsHm1QC1RAFFqLIstNSS5mkVpTxFn6N4oygskC68QGlBgRKViKCBgqd00gdIAGlC+kAC\nIaFnvj/2Ptvfnty97rn3nnPPPvd+PmNkZO279l577rV+a+39O/M352q99wAAAIe3f7HTDQAAAHae\nxAAAAJAYAAAAEgMAACASAwAAIBIDAAAgEgOAtddaO6+19sCdbsfB1FrrrbU77HQ79kdr7Qmttbft\ndDuq1tqzW2tXttYuba3dprX2pdba9ebr3txae9JOtxHYeRIDgD1orX2qtfaQg/A+z2qtvWLqOb33\nO/fe37zdbWHPWmvf31p7Y2vt8621K1prr26tnbAF2z13vr0vttbe31r70WH9v2qtfbq1dm1r7c9b\nazcv6740/PfN1trzVrzPbZL8WpIzeu+36r1/pvd+VO/9m3t47tolNcDBIzEAgGnHJPnjJKcmOSXJ\nNUletgXb/eUkJ/Teb5rkyUlesZFwtNbunOSFSX4myfFJrkvyRxsvnP+wP6r3flSSWyX5cpJXr3if\n2yT5XO/98i1o86TW2vW3+z2A7SMxANiLjb+ittb+sLV2VWvtk621R5T1b26t/V5r7Z3zv/6+buOv\nu621B7bWPjts71OttYe01h6e5JlJfmL+V9/3r3j/Re/FvIfh1a21V7TWrmmtfbC1dnpr7Rmttctb\naxe21h5aXvvE1tpH5s/9p9baU4Zt/0Zr7ZLW2sWttSfVEp7W2hHzz/yZ1tplrbUXtNa+Y0Ubb99a\n+7vW2ufmJSuvbK0dPXyGX2+tfaC19oXW2v9srd2orP8PpR0/t5fjcfPW2svmz72qtfbnZd0vtNY+\nPv/r/tmttRPLut5a+9ettY+11q5urT2/zRwxf/xd5bnHtta+3Fo7rvd+Tu/91b33L/ber0vy35Lc\npzz3FvP3+mJr7Z1Jbj/V/g299w/03r+x8TDJDZLcev74p5P8Re/9f/fev5Tkt5L8H621m+xhU/8y\nyeVJ3rqHffWQJG9McuI8xs5srZ063xfXH557pyQvSHLv+XOvnv/7yjjYiO/W2tNaa5cmeVlr7Zat\ntb+c79PPt9be2lrzewN2AScqwObcK8n5SW6Z5A+SvKS11sr6n03yc0lOSPKNJP91bxvsvf91kt9N\n8j/nf/296ybb8ugkZ2X2l+z3JnlDZtfzk5L8dmZ/ad5weZJHJblpkicmeW5r7e5JMk9MfjXJQ5Lc\nIckDh/d5TpLTk9xtvv6kJP9xRZtakt9LcmKSO2X2A/dZw3Mem+ThSW6b5LuTPKG049eT/FCS0+bt\nmXJWkiOT3DnJcUmeO9/Og+ZteGxmx+HTSf50eO2jknzf/P0fm+RhvfevJnltkp8a2vqWFX9lv3+S\n88rj5yf5yvw9f27+36bMf0B/Jck7krw5ybvnq+6cZJEo9t4/keRrmR2P0eOT/L+99z6u6L2/Kckj\nklw8j7EnrGpL7/0jSf51kn+YP3cjsdtbHNwqyc0z6015cmZlS59NcmxmvR3PzCzxAdacxABgcz7d\ne3/RvC775Zn9CDy+rD+r9/6h3vu1mf1197FtPrhzG7y19/6G+V+bX53ZD7Dn9N6/ntkP4VM3/lrf\ne/+r3vsn+sxbkvxNkvvNt/PYJC/rvZ83/0v4szbeYJ70PDnJv++9f773fk1mScxP7qlBvfeP997f\n2Hv/au/9iiT/JckDhqf91977xb33zyf5i8x+aNZ2bOy/Z2WFNiu1eUSSf917v6r3/vX550pmf2V/\nae/9PfMf+8/I7K/fp5ZNPKf3fnXv/TNJzi1t+B/DZ/tX838b3/+7M/tR/B/mj6+X2V/s/2Pv/dre\n+4cyi49N6b0/KslNkjwyyd/03r81X3VUki8MT//C/Lm1Padktp83/Z77YpNx8K0k/2l+7L+c5OuZ\nnR+nzI/PW/eUtADrR2IAsDmXbizMf0Qnsx9vGy4sy5/OrCzkltvUlsvK8peTXFkGkn65tq219ojW\n2tvnJR1XZ/YDdKNdJw7trsvHZvZX+X+cl4RcneSv5//+bVprx7fW/rS1dlFr7YtJXpFv//yXluXr\n8s/7b2zHp/f0HnO3TvL53vtVe1h3Yn3tvATnc5n9hXtvbTg3yZGttXvNE4m7JfmzuvF5idU5SX65\n975RtnNskuvvQ/u/zfzH8zlJHtpa+5H5P38ps16e6qaZjW+ofibJ23rvn9yX99wHm4mDK3rvXymP\n/+8kH0/yN/PytadvU9uALSYxANgaty7Lt8nsr6ZXJrk2sx9WSRZ/Ya4/qrbtL6mttSOSvCbJHyY5\nfl4a8vrMyn6S5JIkJ5eX1M9wZWZJxp1770fP/7vZfLDrnvxuZp/lLvPBtI8r77M3l+Tb998qFya5\neR2/UFycWTlLkqS1duMkt0hy0d4aME+sXpVZOdFPJfnL+V/HN7Z1SpI3Jfmd3vtZ5aVXZFY6ttn2\nT7l+/nl8wnlJFqVlrbXbJTkiyQXDa342W9tbMMbjZuJg6TW992t677/We79dkh9J8quttQdvYRuB\nbSIxANgaj2utndFaOzKzOv//Nf+xeUGSG7XWfri1doMkv5nZD7wNl2VW+rMd1+Mbzt/riiTfaLMB\n0w8t61+V5ImttTvN2/1bGyvmJS0vymxMwnFJ0lo7qbX2sBXvdZPM/sr9hdbaSZmX2mzSq5I8oey/\n/7Tqib33SzL7q/0ftdaOaa3doLV2//nqP5l/nrvNk6LfTfKO3vunNtmO/5HkJzIrSVqUEc0/z98l\n+W+99xcM7flmZuMTntVaO7K1dkZmNf+TWmvfOe/N+Y75Z3hcZmMXNsqiXpnk0a21+80TnN9O8toh\nWfmBzHpDVs1GtD8uS3Jya+2G88+3r3GQ1tqjWmt3mJchfSHJNzMrNwLWnMQAYGucleTMzEpVbpTk\nqUnSe/9Ckl9M8uLM/nJ9bWYDMzds/Kj7XGvtPVvZoPmPyKdm9sP7qszq5s8u68/JbJD0uZmVfrx9\nvuqr8/8/bePf5+VBb0pyxxVv95+T3D2zH4J/ldmP5c2285wk/09mP74/Pv//lJ/JrEfmo5kNrv6V\n+XbelFly85rMeiFunxVjIla04x2ZHZ8TM0s+Njwpye0y+/G/uHdAWf9LmZUkXZpZDGxmKtOW2ViK\nyzNL3H45yU/03t8zb8t5mQ0EfuX8OTfJLI6qx2dIFrbA32XWW3Fpa+3K+b/tSxwkswHkb8osUfyH\nJH/Uez93C9sIbJNmPBDAgWmtvTnJK3rvL97pthyI+XSVH0pyRJlGE4DDhB4DgMNYa+3H5/PUH5Pk\n9zObO19SAHAYkhgAHN6eklmpyicyqwX/NzvbnEPHfHzAl/b03063DWBPlBIBAAB6DAAAAIkBAAAQ\niQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkB\nAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAA\nEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJ\nAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEA\nABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQ\niQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkB\nAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAA\nEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJ\nAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEA\nABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQ\niQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkB\nAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAA\nEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJ\nAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEA\nABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQ\niQEAABCJAQAAEIkBAAAQiQEAABCJAQAAEIkBAAAQiQEAABCJwaa11s5rrT1wp9vBvmut3bG19r7W\n2jWttae21l7QWvut+boHttY+u9NtZHuJAcQAYgAxsHfX3+kGTGmtfSrJk3rvb9rm93lWkjv03h+3\n6jm99ztvZxvYVr+R5Nze+9329sTtiLnW2s2TvCTJQ5NcmeQZvff/sVXbZ1N2OgZ+KckTktwlyZ/0\n3p+wVdtm03YsBlprRyT5oyQPSXLzJJ/I7DpwzlZsn03b6evAK5I8OMmNk1ya5A967y/equ2zKTsa\nA2XbpyX5YJL/NfXbcyfoMeBwcEqS87b7TdrMns6p5yf5WpLjk/x0kv/eWpNoHlw7HQMXJ3l2kpdu\ndxtYaSdj4PpJLkzygCQ3S/KbSV7VWjt1u9vDkp2+DvxeklN77zdN8iNJnt1a+97tbg9LdjoGNjw/\nybu2ux37Y9ckBq21J7TW3tZa+8PW2lWttU+21h5R1r+5tfZ7rbV3tta+2Fp73fwvtXvsHmqtfaq1\n9pDW2sOTPDPJT7TWvtRae/+K9/9Ua+0h8+VntdZe3Vp7xbw76oOttdNba89orV3eWruwtfbQ8ton\nttY+Mn/uP7XWnjJs+zdaa5e01i5urT2ptdZba3eYrzti/pk/01q7bN7t9R1btV8Pda21v0vyg0n+\n2/z4nt5aO7O19uw9PPesJLdJ8hfz5/7G/N+/v7X29621q1tr72+lpGwed/9Xa+3/S3JdktsN27xx\nkn+Z5Ld671/qvb8tydlJfmabPjKDnY6BJOm9v7b3/udJPrc9n5IpOx0Dvfdre+/P6r1/qvf+rd77\nXyb5ZBI/Cg+SnY6BJOm9n9d7/+rGw/l/t9/qz8qerUMMzJ/3k0muTvK3W/4ht8CuSQzm7pXk/CS3\nTPIHSV7SWmtl/c8m+bkkJyT5RpL/urcN9t7/OsnvJvmfvfejeu933WRbHp3krCTHJHlvkjdktj9P\nSvLbSV5Ynnt5kkcluWmSJyZ5bmvt7kkyT0x+NbMu5jskeeDwPs9JcnqSu83Xn5TkP26yjYe93vuD\nkrw1yS/Nj+8FE8/9mSSfSfLo+XP/oLV2UpK/yuyvvTdP8utJXtNaO7a89GeSPDnJTZJ8etjs6Um+\nMbzv+5PoMThI1iAG2GHrFgOtteMzuzZs+18umVmXGGit/VFr7bokH01ySZLXH/inYzPWIQZaazfN\n7Dfir27Rx9pyuy0x+HTv/UW9928meXlmCcDxZf1ZvfcP9d6vTfJbSR7bWrveNrXlrb33N/Tev5Hk\n1UmOTfKc3vvXk/xpklNba0cnSe/9r3rvn+gzb0nyN0nuN9/OY5O8bP6XhOuSPGvjDeZJz5OT/Pve\n++d779dklsT85DZ9Jr7d45K8vvf++vlf+t6Y5N1JHlmec+b8+H1jfvyro5J8cfi3L2R20WB3ONAY\nYPfbshhord0gySuTvLz3/tHtbTZbaEtioPf+i5ld/++X5LVJvrqn57GWtiIGfifJS3rvazvIebcl\nBpduLMx/RCezH14bLizLn05yg8x6F7bDZWX5y0munCcsG48XbWutPaK19vbW2udba1dnFkQb7Tpx\naHddPjbJkUn+cd5tdXWSv57/OwfHKUkes7H/58fgvpklpRsu3PNLkyRfyqynqLppkmu2tplsowON\nAXa/LYmBNqs5PiuzMUe/tC0tZbts2XWg9/7NeVnpyUn+zdY3lW1yQDHQWrtbZtUhz93eZh6YtZ6V\naD/cuizfJsnXM5sF5trMfmAnSea9CPXHdd+uBrXZbBSvyazM6XW996+31v48yUYJ1CWZXRw21M9w\nZWZJxp177xdtVxtZMsbChZn1RP3CPrymuiDJ9Vtrp/XePzb/t7tGCcE62+oYYPfZ8hiY9wC/JLNe\n7kfqWVp7B+M6cP0YY7DOtjoGHpjk1CSfmVfBH5Xkeq21M3rvdz+Adm6p3dZjsDePa62d0Vo7MrMa\nrv81/yv+BUlu1Fr74Xk37m8mOaK87rLMSn+2Y3/ccP5eVyT5RpsNmH5oWf+qJE9srd1p3u7f2ljR\ne/9WkhdlNibhuCRprZ3UWnvYNrSTmcuyPGDoFUke3Vp7WGvteq21G7XZYPaTV7x+ybys7bVJfru1\nduPW2n2S/GhmfzVkPW1pDCRJa+36rbUbJbleZl8EN2qtHWp/mDmUbHkMJPnvSe6UWc3yl/f2ZHbc\nlsZAa+241tpPttaOmr/+YUl+Kms6AJUkW38d+OPMEsG7zf97QWZjFtbqN92hlhicleTMzEqObpTk\nqUnSe/9Ckl9M8uIkF2XWg1Dru149///nWmvv2coGzccFPDWzBOCqJP8qs1lpNtafk9kg6XOTfDzJ\n2+erNuoOn7bx7621LyZ5U5I7bmUbWfJ7SX5z3k346733CzP7If/MzJK7C5P8h+zbufOLSb4js0Ho\nf5Lk3/Te9Risr+2Igd/MrPfv6ZnVqX55/m+spy2NgdbaKUmektmPgUvns5x8qbX209vTfLbAVl8H\nemZlQ5/N7LfAHyb5ld772ZOvYidtaQz03q/rvV+68V9mpcZf6b1fsU3t3y+t90OjB7y19uYkr+i7\n/GYhrbU7JflQkiPmA5sBAGDbHWo9BrtSa+3H2+x+Bcck+f0kfyEpAADgYJIYrIenZFZm8okk34xZ\nCgAAOMgOmVIiAABg/x1Qj0Fr7eGttfNbax9vrT19qxrF7iEGEAMk4gAxgBg4FOx3j8H8XgAXJPmh\nzEbZvyvJT/XeP7x1zWOdiQHEAIk4QAwgBg4VBzKP9j2TfLz3/k9J0lr708ymcVoZAK01dUs7qPfe\n9v6sfbLPMXDjG9+4H3PMMVvcDDbjqquuyrXXXrvjMXDkkUf2o48+ep/faH5DmL2qf+zY7Gv21/iH\nlfp+61imefXVV+e6667bjp2yT3Fw5JFH9pvd7GZ73egB/OFqU8+bOn6bbcu+xNhmP89UHB1ofH/x\ni19cixjwXbCzLrrooit778fu/Zn7ZJ+vA/vzXbDV9uU6sFPb3442XnLJJXuMgQNJDE7K8q2fP5vk\nXgewPXaffY6BY445Jv/u3/27bW0Ue/a85z1vOza7zzFw9NFH5ylPeco+v9FWJAb7+8N91Xt/61vf\n2pLtV1OvO9Avgxe+8IUH9PoJ+xQHN7vZzfKEJzxhrxsd9+9m/Yt/sbkq2XH7U69b1ZbNvtfUNqa2\n+Y1vLE9Q981vfnPle9fHq2Ll5S9/+abasB/2KQaOOeaYPPWpT93SBmxFMl7327gPN7v9zSZv+7v9\nrfC0pz3t09uw2X2KgaOPPjo///M/vw3N2DdTP7q34hjty/ZXvW47EoNnP/vZe4yBbZ+VqLX25Nba\nu1tr797u92I91Ri49tprd7o57IAaA9ddd91ON4cdIAbwXYAYWH8H0mNwUZJbl8cnz/9tSe/9jzO7\nDbRSokPPPsfAySefLAYOLfscAyeeeOJ+xcDUX0i+/vWv7/F517/+8iVuahtTf82tf6X96le/ulge\nt3fkkUculse/5tbtb/avTlN/jV6zUqW9xkGNgRNOOGFl46f2U/3r+dRfy+vz6rEbjdvf7F/0r3e9\n661cN/V+dd24jRvc4AaL5a997Wsr23TEEUcslsf4q9vc3892APYpBqa+C2pbxx6TepzH83tVj8m+\n9OoM7Z18vOrf9/f9NvNea26fYmB/vwumrNpvU72D4zlbz8XxHKvxWM/T8Xl1G+O5PtWzXL/Lapun\ntjEVb/vTs3Ag0fuuJKe11m7bWrthkp9M4tbehxcxgBggEQeIAcTAIWG/ewx6799orf1SkjckuV6S\nl/bez9uylrH2xABigEQcIAYQA4eKAyklSu/99Ulev0VtYRcSA4gBEnGAGEAMHAoOKDGAg2Vqloet\nnkZwu6e45J9N1dFO1feOdcdVrcX88pe/vHLdqG7zO77jO1a+rtZ4j2obx5rWqfeu9ah1G7XedG92\naU3yt9nsuIrxeXV8Rx0HMrWN8ZjUWuPx+NVjVJe/+MUvLj2vXj9udKMbLa2b+my1/dXYjqk21sdT\nYx3Wxapr7dS5UvfhuH/rmIOpfV3jY9xP9TowVZe+2e+JgzC2g2JV/f7UmKSpa/V4Lfnc5z63WL7m\nmmtWbr9eu8dB1ieeeOJieTzv6+Mb3/jGi+U6niHZ/Pm9P7MZbfusRAAAwPqTGAAAAEqJ2B2mpvda\nNYVh7e4fXzfVvbudN5hi2b7cgKx2nU5NDVdfd8Mb3nDpeVPTQNZtjqVK9fFRRx21sh21a3ncRu2e\nHksgqlVTrybLpRKHaonCZj/X1Hla76o8NaXl1HuPXfe1LK1u//jjj196Xj1+U+VwUyVCNVbGMoSv\nfOUri+UxPqbKjMb9sM6mpmKs587UeVS3MZZ7XX755Yvluj9HY9ngTW9608Xy1HSUNXamplsdP1vd\nju+ag2c8fvV747jjjltad8oppyyW6zXhVre61dLz6vfEBz7wgaV19dy8+uqrl9bVMtbarqnr2Fbf\naFOPAQAAIDEAAAAkBgAAQIwxYE2NtZe1hm6s2Vw11eg41eNUjfrUdKWrps7b3+lQN7ud/ZlmbLeZ\nOg5TUwWOx6ROKTc11Vx9v3GauLr9sb681hPXutJx+/XxWJ+82drwGrdjDXX9bON0dbUG9VCZunSq\nDn88Rl/60pcWy/XYjtuo+2Ych3SLW9xisXzLW95yad1ll122WL7ooosWy0cfffTS82pt8XgcagzU\naQ/Hx3Wb49S59ThfeumlS+tqDI+vW9WmnbTRlrF+uu6n8Xp/xRVXLJbHc/ghD3nIYvmqq65aLNex\nP8nycf/CF76wtG5qnEY9p+t5Ol6P6uvGz1a3sRumlF1HWxHDU1P71ngZf0fU86pex88///yl593z\nnvdcLN/tbndbWlfjpV5XkuX4rt8n43fGVPzV/WO6UgAAYL9IDAAAAKVEO+3BD37wYvmVr3zlYvkB\nD3jA0vPGbqpD0dSdbeu6qa7ZqbvGTnXvTnXZrXrd1PR/U9MUjmo3YC0FGLs393d6ynUrQWqtrWzT\nqqlnN163Sj3Wdb+NpSLVOE1cNZYB1Xbd/OY3X9nGWmY0xl8tMRmPbZ1OsZYPjdOt1u1P3R12NxmP\na92nU+fNeD7U8pO6n8Z9WLc5TlX5yU9+crE8luLU43LllVculsdrVT3uN7nJTVa2eTx+dVrEGn/j\nNmr8rbpb8p5slF6tY9yM19J6bo4lXR//+McXy29+85uX1tUSkGOOOWaxfNJJJy097y53ucvKttTj\nN57DNa5qycd4PtfHdWrbcd1Y4rRqmtap78NDxb6UvNR1U6W/9Xt9LD2s68aS0M9//vOL5anv3Vr+\nN15LPvrRjy6W6x2Mk+SMM85YLNfyxWR5StzajvEcqZ+7xmVy4Oe4HgMAAEBiAAAASAwAAIDskjEG\n97///RfLYz3Wn/3Znx3s5myp7/u+71ssv+td79rBlqy3Wk831trVaQrr86699tql59UaxlrTnSzX\noo/1gLWOty6PNYVTU6rWWtWx/bUGutZBXnfddVll3MbUrdN3Uz1q3Rdju+u+GWt66/6tx7LWGSff\nftyrOkXkODZh1dSH43Gu16exRr22cZxmssZt3cYYA/X9pqZIXLdxJftiauxO/Yzj+I563tZzc9xP\ntX5/nKqyHufx+nGrW91qsVyvAxdffPHS82p8jJ+ljhcYv8tWfe4xZmtcjfXr4+epNvbdOl4PpsaD\nPelJT1paV8+ds846a2ndc5/73D1u/xnPeMbS4zqeY2p8wLg/67Goba7jh5Ll2BzHiNT4GMe/1Pde\nx+N0MO3L+KKqXjPruT9ej48//vjFcv0NkSyPJxpj4IILLlgs11gcpy3+p3/6p8XyGGMXXnjhYnn8\nvVHP6TreoC4n0+Ooqqlpv1fRYwAAAEgMAACAXVJK9MAHPnCxfNpppy2t222lRGM3zm1ve9vF8imn\nnLJY3s2lANuhdsWNXXa1y7F2qY0lBLXrcCzRmLrr7aqpv8ZtbPbOs2PXYd1OLV8Yt1E/21jCso5T\nEK7Se198trHd9XONx6HGwLgPa0zUc2fsIq7bHKd6nLoTZi09GKcYrGoc1Sktk+Qzn/nMHrc3Pq7P\nG68XtZt5nFK1tn83lyFMnUe1i3+8DtRj++lPf3qxPJZ51GM7Huda3jOW+oylRRtqiVGyPBXheOfq\neozGOx/X59YylfF5tXxhPH+mzpGN68f+Tnu81Vpri/geS/fuete7Lpbr9KxJ8vznP3/lNuuxrneb\nHUst3ve+9y2W6/duslwC8sEPfnBpXb1W13NxLFms5+YYR7Ut47Go17+p6Y0PRfv7m2eqNKbGznjd\nnpo2tl53xuP3gz/4g3vcxtj+Y489drFcr+nJclyNx7Ye9/odNXUH9PH3QD2fxv2zmfNfjwEAACAx\nAAAAdkkp0c/+7M8ulv/hH29S6yEAACAASURBVP5hB1ty4E444YSlx7/wC7+wWH7FK16xWK53zTtU\njV1atftuqsRkLC+osz7Uu2SOd7atXWpjt1zd/tSMQvVOhGPXYZ0lYCyDqWUPY7vqrAe1e3C8G+Oq\nkqlkuVt73HebmYVgp4wxUPfbWCoz1bW+ajanqW7a29zmNkvrVt09edx+7aYdu6A/8IEP7PE1yXLJ\nQr1rarI8S0U9zuMdW8e4rXZr+dBUmcQ481c9V6buaj01G0k9v0899dSldbXrfjyPLrvsssXy5Zdf\nvlgeZ5iqx3YsVxvLn6pLLrlksVxL4Kbu9D62sV4LxzKmje2sY5nqeM2tx+xFL3rR0rpa2nH22Wcv\nratlHn//93+/WP7TP/3Tpeedfvrpi+WxFKyWgNz97ndfWreqfGO8ptfyr/EacdFFF2WVsWxlw249\ntw9EjdOpzz+eA3X2nqnX1e/XsUywHrPv+q7vWlpXj1H9bq2/DZLla/cYR7XMbVQ/z3ve857F8vhZ\nNnuH+PE6qZQIAADYFIkBAAAgMQAAAHbJGIN1rpHeVy9+8YtXrvvYxz52EFuyfmqt8VhbXOv3x3q6\n+rjWDY7bqHWDY61vHZswTvNXa3Vrvd5Yq1vr/se7JU6NfbjDHe6wWK7TLH7yk59cel79nGP9cDXW\nXK5jfepGm8Z9WGuyp6ajHO8Gu2qa0zpFbbJcgz01fd14/GrNc63RHKesrfu6HvMk+fEf//HFcq1X\nT5KXv/zli+U6JfN97nOfped96lOfWiyPYxh2q6n4HMfS1PN7/Pz1e6JOUziOVTnxxBMXy2OM1XUn\nn3zy0ro6jqfG1Xie/vVf//Viebzr7Yc//OHF8u1vf/uldW9729sWy3W8yzgVZt3mGH9TYys2rMsY\ngzpt8VTd8zjl8OMf//jF8nitq9NC1jEG4/Xy4x//+GJ5rPmvx/ae97zn0rrb3e52i+Vaaz62v7aj\n1okny/E+vq5+96zjdXs7jXFZP//UOKTxrtl1HEg9RuNU9/X8Hq8R9Zx+17vetbSuTmdb42/8Pqlj\nhsYpces5XH/bJMtjXOo1YhzDUF83tX/2Z6rbvf7ibq29tLV2eWvtQ+Xfbt5ae2Nr7WPz/x8ztQ12\nNzFAIg4QA4gBxMChbjN/ij8zycOHf3t6kr/tvZ+W5G/njzl0nRkxgDhADCAGEAOHtL2WEvXe/3dr\n7dThn380yQPnyy9P8uYkT9uqRn33d3/30uOxHGA3q93Roze+8Y0HsSWbt10xMHYd1i7AsXuwqlMF\nJstdzbWLbpyCbOpuovUutWMpUe2CrNMbjtMI1jKEsYypliyMXdd1erWHPOQhi+Xxjp8XXHDBYnks\nR6plaOO6rbrT6VbFQb3j6RgDtRxknMKwHqOxhKCWmNTykzGOahfxeIzq+43lG3Uf1rga21hLf+rd\nW5Pl+Kh3c0+Sn/iJn1gsf+ITn1gsf+QjH1l6Xu0W3ok72G5VDPTeF+2fuvvr2MU/1X1eSw9qt/vF\nF1881ZQlteRkfO+67i53uctieZz2th7ncXrqus1xysIf+ZEfWSy/4x3vWCzXqWyT5fgbY7iWV42x\nuVWlKVv5fbDRprGcopbfjJ+xfofWkqBkuczjYQ972GK5nlPJ8nfGWKpUp5t95zvfubSulnvV6/bU\nHa7H7U9NOVzjY51LBbfyOrARA2N8Tt2Jvl7vx++7Gi/1PBqngq3n6Q/90A8trbvHPe6xWB6vA295\ny1sWy7WkdTzO9Xu+fncny9/t43S5NZbq99A4fXktXRrP9fG3SbWZ0vz9Ld4/vve+UUB1aZJD55c7\nmyUGSMQBYgAxgBg4ZBzwqN4+S/NW/imitfbk1tq7W2vvPtD3Yj3tSwyMf8Xn0DEVB2Lg8LDZGBj/\nQsyhw3WAzcbAOHie9bC/icFlrbUTkmT+/8tXPbH3/se993v03u+x6jnsSvsVA2OZDrvepuJADBzS\n9jkGpsop2JVcB9jnGBjvDM562N/pSs9O8vgkz5n//3Vb1qIkj3zkI5ce7/YvkTpG4ra3ve3K503d\nKn0N7XcMrJqirj4epymsdXdT4zRqLeIYN7UmdFTr1+s0Y0ly1VVXLZZrzfr4V89xbExVvwTHetda\n01prksfa5Vo3ONYU1r++jfWY2zw94T7HQa0vH+tK69iBsWazPneskzzqqKMWy1PTjtbaznF6uRov\n4/6tx6y+V51aLlmehvQv/uIvltbV2texJvl7vud7Fst1StJx/4znxZo4oO+D8VjWzzgeo3r+HXfc\ncUvrLr300sVyrTuemrb4s5/97NK6+rpxfEo9x25961svlsepCOv16aSTTsoq55577tLj+973vovl\n+tnGH0+1/VPjM8brwDZP+31AMTCeb/XaOtaG13EEZ5999tK6Ojbhfve732K5Th+aLB+zsZa/7tP6\nvZAkH/3oRxfLNa7GY1SPX61lT6bHTNZjtj/TTO6wLf1dWL/vxu/yeo2YGmdTj+U4bWwdL1LHIiTJ\nIx7xiMVyHfeXJA9/+D+Pua6/2er3eLI8Pqxem5LlMVDjunr9r+MPxrEINY7G78qp2NnM74HNTFf6\nJ0n+IckdW2ufba39fGYH/odaax9L8pD5Yw5RYoBEHCAGEAOIgUPdZmYl+qkVqx68xW1hTYkBEnGA\nGEAMIAYOdWt55+M73vGOK9edd955B7ElW+MP//APF8tjN2KdxmrsNj/c1K7DsWSidoOP03bVuxHX\n6evGLuLaXT2WCdRyobFrsr537Tqsr0mSd7/7n8fXj911UyVk9W7HtetznOqwlp+McVS7WmupS/Lt\n06+us6lSiKnu47pv6r4fn1fPt3vd614r21G7mcft13aN0xnW0pRxKsUat2PpQS0bqzEw3nm1li6N\n26jnwW4yTq1Xz9vxPJ06B+q6OsXweJfsOlVxLQlKlqezHc/vWmJYS13GKWVrKdF4N/v63mNpz1/+\n5V8ulmupy9jGuv2xJG2qTGBjP6/jHXXHGKifcbx+/eM//uNieTxG9Ryr5+ZY9lkHvU6VEo2DY+sx\nq1NXj3e4nroe122MpWD12rKOx+lgqvtm3E/1u2As1ap3nK/HbyzNrb8lP/jBDy6te9/73rdYPuec\nc5bW1d+n9Xo8ljvVuB3LDeu1e/yeq7Ffvw+nYnH8TVTX7U8J4bYWHQIAALuDxAAAAJAYAAAAazrG\nYMq73vWunW5Ckm+f+rJOYfW4xz1uad1DH/rQldv5nd/5ncXyeGvvQ9VGHexYV1rr6cZa2TqN5TgF\nZZ0mtNYXjrXFtQZwrCms4zs+/OEPL62r9am1HWON96pbmSfL9aLjjX1qzeLtbne7xfJYH1/fe6qm\ndd2nuWutLeoexzraWks6NZXpWE9f64RrfIz12Xe6050Wy+MUb6973T/PrjeOCbn73e++WK77eqz7\nrONFHvWoRy2te9CDHrRYHqfJrLXttX52nO6y1otu8zS026rGwKgey/EaUR+PteF1Ksk6Le04TqE+\nHmv063Ef19UxPzX+6jUnWT43x3FjdYzBeJ7W60J977H99bhPTeVdz5f6fusYN2Ms1OM8rqv7dPwe\nrlM61uvzeJ7WYzZex+t1Z/yuqa+r7RrHxNVtHnHEEUvrpq7P637tPpjq74Gpqc1Htd6+nqfjca5j\nAH7sx35saV39LTZOX17HEtbn3fWud116Xr2WjOMb6rk+fpfVc7pe08bzuRr3x4F+T+gxAAAAJAYA\nAMAuLCWqXcT7onbzjF0r9c52J5988tK62kX40z/904vlsXuzlpuMd9GrXVtj13ideu1wsdHtNVUu\nNE4vV0tCxrtk1pKQ2j04ThP3gAc8YLH8cz/3c0vratfe+Lp6p83arT928U919dXjXMsJRnWqsnH/\n1C70scymPh7XrXP39LjP6rEdj3PdH2O3cC27qsdlLLWo+/7P//zPl9bVae9e9rKX7bXtSfLEJz5x\n5boPfehDS4/rVLc1ppLl0rNaDjFeZ+qxnepO302mYnlU42V8Xp2+tF5nx3Keek0f46NOKzju+1qO\ncvHFF++xTcnyOTzeGbses/EaV78n6ntNlUSOZQhT5/o6T385dUfv8TpY99O478dpIVep59t4Lan7\ncCwnq8d26po7NUXkOh+Hg621trLUpe7D8Tu5fjeM+7OeE7UUrN5ROEnOOOOMxfIDH/jApXX1O2Sc\nLrduv56n4/ZrmdF4np5++umL5anvofq9Nl5L6rrxPDjQu5zrMQAAACQGAADAmpYSjV2stavoBS94\nwdK6Zz7zmZvaZp25Zuy6mrq7XJ2h5qUvfeliuY5MT5K3vOUti+XLLrtsaV2dgWTsuq6jzg8HvffF\n8Rxn3and/2OZTi0XGmeiqOtqt/B4N8DnPe95i+Wxq63exfI7v/M7l9bVruW6PMZKjatxVqLaDTh2\nm9d4qctTXc5jGclYorbOagyMx7l+5nE2j9pdOp7D9XEtO7v88suXnlfPtx/4gR9YWlfvUP6Zz3xm\nad1zn/vcPW5jvEv7ve9978XyG9/4xqV1NcbGGKiPp+74uY4zyuyP3vvis03d/XU8T+tMROPMXPUc\nu8UtbrFYPv/885eeV6/B47q6zTE26zldr0HjLGP1ujZuv8b0eM7W140zZlV1H4zXuGrc/qqZwHbS\nqrbUGBhnn6qvmSrnnCqruvDCCxfL4/dQLfsYS4lq7NRzdtzXtV1jSaQyo82p14VxH9b9Pf6mqjO5\n1TLCWgaWLJcWj2VANT7Gc6zONlSvx+PvgRof47Xq0Y9+9GJ5LGUbS4v29F7JcpxOzeA3NfPfKnoM\nAAAAiQEAACAxAAAAsqZjDH7xF39x6XG9m+hYF7xZtWZ4rOH6yEc+slh++9vfvl/br5785CcvPa61\nr+M0hYebOj3ZOIVXrdUd6+JqzWatH06Wa8rrlJB1OUne+ta3Lpbf+973Lq3b7JSeU1OoHXfccYvl\n0047bWldrYkcxx/U6dDq8liLWuuTx+nJat3g+Lp1rkufmmpurMOvtcBjXXf9jHVfX3nllUvPu8td\n7rJYvvOd77y0ro4JePazn720ro57qlPPjnH0/ve/f7F89NFHL62rYwzGOs96bOvxOxxqjqfO9fFu\n8HW62fHY1v19n/vcZ7F8j3vcY+l5dcrrOu1oknzsYx9bLI91zTU267k/Xgfqsay1ysnyXVTHMTSr\n7nw87p+pO2/XdWPd+zpeBzbatC/jqeq1b7xG1M9cl8djVK/343Gox30co7RqjMgYK/Xx1H4/HM7v\nKXW82VTt+3gdqOfi5z//+aV19Ryo19zxDuX1++Sss85aWnfppZculscp8uvv0Trl6RhjV1xxxWJ5\nHMPwXd/1XYvlRz3qUUvr6nSlr3rVqxbLY5zW76SpKZ/H3wqbiTk9BgAAgMQAAABY01Ki0e///u/v\ndBP2yYMf/OCV617zmtccxJasp42u1bGru3YL1264ZHn6vtodnyxPGVm74Mfu3VqGNpaR1G788c7E\ntcuutnmqi3+c5q4+Hru/6zRqU+2v68Z9N9U9uG7d1bWcbKqbc+xartPGTd01u76udssmy9PG1bti\nJsvdu+Odj2v5UI3NsXv3bW9722J5PM61RGjqbqvrdry2Q2ttsT/G0re638YSv3pOjN3z9Q7BF1xw\nwWK5lo8lyWMe85jF8oMe9KCldbXktJYMJMtlRrW0oU6PmCR3u9vdFst1WsIked/73rdYHo9zPadr\nWdRYRlH3z3hX5Lq/xvNn4/3WpaRoKgZqqchUaeQ41Wjdp7XMaiw7u/3tb79YHq8R9Tozlp/Uc79e\nP8Zrdd33U1NJHw7n+maN3wX12jrGbC31GaeHr+VftVTw/ve//9Lz7nvf+y6WTz755KV1tex4vIP9\nJz/5ycXyX/3VXy2Wx++Tet7W9o7b/OEf/uGldfX69MhHPnKxPE7jX3+z1Gn1k+XzYtx3SokAAIBN\nkRgAAAASAwAAYJeMMTiU/Nmf/dlON2HHbdS8jbVvtaZwrMGuU5KN9aL1cZ0ybKw/rdsfb49+q1vd\narE8TjVaa1xrDW8d95AsT2s21kvWesCpmtlaFzvWqNdpHMdtjI/X3appCqemrKtjLMbPW49LXTfG\nWK3JHmvIP/GJTyyW63iDZLletC6PtcVT0+rWzzZO1Xs41xqPx6jWZI9jdcYpAVd5z3ves1g+55xz\nlta9+tWvXizX6YGT5drzcbrZes2o14vxXK81yGN7v/d7v3fl62ps1hgbx1nU6854vtTHq6ZgXsdY\nG9ta6/zrcrI8BmwcY1G/C+q4oPF597znPRfLt73tbZfW1fFn9XxOlq/P9ftkjNOpa1A1db07HNTx\nZqOpKV/rWLHxe/68885bLNexAm94wxuWnvfQhz50sTxe7+u5P44/qNf1+t7j1MF1LOQ4pWqN0xe+\n8IVL6/7mb/5msVx/i9Rp2ZPlafDH86c+3p8xRbvr1wQAALAtJAYAAIBSInbO2KVdu9ZvetObrlw3\nTttVt1O72afKhcZuv9olOLardmnWrt/jjz9+6Xm1BOKqq65aWle7nWs3aLLcVV63P06FWds1tvFA\nuw4Ppt774nOOXemryrbG5262lGo8zrU0YLzrbX3uWHpQu5ZrGdDYxlqKMpYL1fYf7iUENQam9tO4\nf+uUwONUkvWaUUs7xukM67k+lizWa8tYZlSnI6ylgePd7Gupy1ve8paldbX7f9x+jeE6Beo43eVU\n7NTr5Gbv5r5TagyMJXnVWI5V42Us16v7sJZtjdeHej7X5yXL1/UxduoxG+9gX11zzTWL5anPxmpT\nJW81Jsbvyfr9V0t4xml/X//61y+WxzvYH3fccYvl8fpUr0G13Hf8zVJLi+v1Iln+zh+nR6/XmTpF\n8ngX9anvmqk7p2/mu2evPQattVu31s5trX24tXZea+2X5/9+89baG1trH5v/f/VZwq4mBhADiAHE\nAGLg0LeZUqJvJPm13vsZSb4/yb9trZ2R5OlJ/rb3flqSv50/5tAkBhADiAHEAGLgELfXxKD3fknv\n/T3z5WuSfCTJSUl+NMnL5097eZIf265GsrPEAGIAMYAYQAwc+vZpjEFr7dQk35PkHUmO771vzMd0\naZLjV7zssFdr3k4//fSldW9/+9sPdnMOyMGKgbFmrtYTjzX0tW681tXWet5kuf72a1/72sr3Hmt/\n6+tqbeNY+1rHN4y1y7X+cJymsL5uqq6y1kZP1a1u93SEWxEDG22cmmZ16nOMdZKrxlVMjdO4wx3u\nsHLdOE1hPWb1vcf67/Hxqjav45SR++JAY6C1tjj247Gr9bfjcajn3FhDX+vy6/l397vffel5te54\nnOpwHJdU1fO0nn/j+TxeP6o6lel4Dtfa9np9GmOq1jjX5WS55nncrxvt3KoxSFt5HRiv91NTgU5N\na12nIa3HcrxeXH755YvlWtOdLNdyj2ON6rGuy2P7a1xNjYcaj8Vuuy4caAz03ld+F9TH4/Gr5874\nujq9aP3dMI5NrOfYePymvq/rc2u7xmtJ3f7UdKvj2IT6ulXTcO+L/Xndpl/RWjsqyWuS/Ervfels\n6bMju8eIbq09ubX27tbau/e5dayVrYiBcRAOu8tWxMA4IJjdRQzguwDXgUPXphKD1toNMguAV/be\nXzv/58taayfM15+Q5PI9vbb3/se993v03u+xFQ1mZ2xVDEz9NY31tlUxMP6Vj91DDOC7ANeBQ9te\nS4narA/kJUk+0nv/L2XV2Uken+Q58/+/bltaeAio3YO77Q61yc7EwNTdgccu+LquXmjGbUxNIVe7\n88YSofrc+mU23vl4qvu/djOOZUy1u7q+bqqcZez6rLajO3qrY2Az5QxT5UL7+xlrGcJUG8btr+r+\nH9u4m6aN3VfbdR0YS4Jq3I/d+HWKyKm7i9fzbZyO8lOf+tRiuV4TkuSd73znYnlqmsJa3jKWH9Up\nNKeuJVMxXN97qpTmYMfYdl0Hxs9R99NY6lM//9QxqjEwlnnUcrJx+/WutGOvxng3+g3j9b7G1Tg9\n9W5PiLYyBlq58/F4Pozff6tMTWtdj9d4LtbrxxgfNY6mvstrj8f4vHrejtuvzx1jv8ZHPQ+myp/H\nuJy6LmzmmrGZMQb3SfIzST7YWnvf/N+emdnBf1Vr7eeTfDrJYzexLXYnMYAYQAwgBhADh7i9Jga9\n97clWZViPHhrm8M6EgOIAcQAYgAxcOhz5+OD7N73vvfS4zPPPHNnGrLL1O7CsQyodh1OlRzVWQnG\nLuJV3YPJcpdgnTHl0ksvXXpeXTeWF5x44omL5XGmi/q6qdksNns3w3HdoVbSsiebLS3a7L6Ymi1k\ns+91OB6H/TGWAtT9NHb/1+eOdz7+6Ec/uliuM/xMlXs9/OEPX1p361vferE8zmhVt3nuueculqdK\nQ+q5PRpLhGq7aqnB1F3Od9ssNqONYz3GQL12j9fx+tzxGlxnG6r7cIyjWpI2xlE9nuOdj2s5x6qZ\nypLlEqdV5UeJa0I1Fcvj7Fv12G72LvJj6eGq8y1Z/u4dS5rq74N6bMc21u1PzY43WnXn9/H3QC2b\nnpq1aX+uEbuv4B0AANhyEgMAAEBiAAAAGGNwUKgj3JypWri6bpy2q9b81X09Tida6/enpvscpxZb\nNWXYWBdbpykcxxhcffXVi+XxDoyrpkYbawprjeRYuzw1Ja74Y52N5/3UNIJT4w8e9KAHLZbrGKJx\nTM/555+/WB7nUR/PzareUfW0005bLNepL5PluuOpsUDj9WPVOKrxeXWbU/XVu2Fq7FXX/KlrVq35\n3uwd4McYqOM7xjvP1jEGxx577NK6up0aO2M7dvuUpOugfhdOjUOaunP61N2Hq6nfHuPvjTpeoH7n\nT53r47lYY3g8h+s1r66bGqcwxl9dtz/f/+t/5QAAALadxAAAAFBKtB3OOeecpcePecxjdqglu8tU\nl1ftihu7/1d1247dj7WLfyz1qcbpyWp3Xi01GLsOa7fi2D1YH4/br9uZmoqw7p/xs02VIO32KQ05\nvNSu9M997nNL6+q5U+9QmyyX+tRu93Gawnp+XHzxxUvr6vSUU3ddrsujWsI4lvxNlSKustnpGJPd\nUT60J2O76+PxOExNZVrV/TtOT11jZ5w2th7bqXZV43SXtc2bPc6Hu82WjyXT03PX5666Y32y+eMy\nTkM6lp7tqU3j9scp0Gu7pqYjrr91xvZP3Qn5QMuHd+dVBAAA2FISAwAAQGIAAAAYY7AtzjzzzMnH\n7Lup6bfq41q7N3Wb+rE2tdb1TdX01lrS8Vb3tbZxrP+r7RpfV+tRp2oDa7umbuFuTMF6clz2bIz5\nep7e5CY3WVpXp/0dxwd89rOfXSzXcUfjeIDjjjtuj9tLkiuuuGKxfOmlly6te+9737tYrmMYxjFP\ndYrLcSxT/WzjdaCuq9eEw3364an4GMcfrJq2edyHdRvjuqlpJut4hKlpY6dq28d6+cNZ732xHzc7\nxnDjdRvGsQI1Juq5OX4nTx3n+v06tquOM5zaxtT3+tT06/V1dXk7xxSM9BgAAAASAwAAQCkRh4Cp\nbsVVxudNlQ/V7deuvfE1U+VOq7Yxmrqj42bvDA27ydgFX8t0xpK5Wsoxnn/1ubVUcLzLeX189NFH\nr9z+7W53u722fU/q+T3erXRV2eOeHu/pNePz9mUq03U2NWXj/pZJ1HgYr7m1jGuqNHXcv6umjJ66\n/h5upV/7orW2qf2zL9+nq6aKHcuH6zanStKmph7f3zuPT5WarWr/wfyO12MAAABIDAAAAIkBAAAQ\nYww4hE3V+G12TEGyukZ0fN5mawCnpl7bn+3BoWqs/a3TD05NEVlN1SePYwDq463Y/mirxwRMXeMO\nFVtxHZwaR7Av772qLVPXdNfx7TV1/tUa/e2Y9nd/z7+pqcerzY6Z3GqH/lUFAADYK4kBAACQdjC7\nuVprVyT5dJJbJrnyoL3xaodTO07pvR+796dtLzGwkhjYOYdTO8TAnh1O7RADe3a4tWPH40AMrLSj\nMXBQE4PFm7b27t77PQ76G2vH2liXz6wdO2ddPrN27Jx1+czasXPW5TNrx85Zl8+sHTNKiQAAAIkB\nAACwc4nBH+/Q+460Y+esy2fWjp2zLp9ZO3bOunxm7dg56/KZtWPnrMtn1o7s0BgDAABgvSglAgAA\nDm5i0Fp7eGvt/Nbax1trTz+I7/vS1trlrbUPlX+7eWvtja21j83/f8xBaMetW2vnttY+3Fo7r7X2\nyzvVlp0iBsTATsXA/L13PA7EgBgQA2JADMz4TbB+cXDQEoPW2vWSPD/JI5KckeSnWmtnHKS3PzPJ\nw4d/e3qSv+29n5bkb+ePt9s3kvxa7/2MJN+f5N/O98FOtOWgEwNJxMBOxkCyHnEgBsSAGBADh3UM\nJDseB2dm52MgWcc46L0flP+S3DvJG8rjZyR5xkF8/1OTfKg8Pj/JCfPlE5Kcf7DaUtrwuiQ/tA5t\nEQNi4HCIgXWMAzEgBsSAGDjcYmAd4mDdYmBd4uBglhKdlOTC8viz83/bKcf33i+ZL1+a5PiD+eat\ntVOTfE+Sd+x0Ww4iMVCIgSQ7HwPJDu57MZBEDJwaMSAGDr8YSNYvDvwmiMHHSZI+S8kO2vRMrbWj\nkrwmya/03r+4k21hRgyQHNx9LwbWkxhADHA4/yY4mInBRUluXR6fPP+3nXJZa+2EJJn///KD8aat\ntRtkdvBf2Xt/7U62ZQeIgYiBrFcMJDuw78WAGBADYuAwj4Fk/eLAb4Ic3MTgXUlOa63dtrV2wyQ/\nmeTsg/j+o7OTPH6+/PjM6rq2VWutJXlJko/03v/LTrZlh4gBMbBuMZAc5H0vBsSAGBADYiDJ+sWB\n3wTJwRt8PB9A8cgkFyT5RJL/8yC+758kuSTJ1zOrYfv5JLfIbKT3x5K8KcnND0I77ptZd9AHkrxv\n/t8jd6ItO/WfGBADOxUD6xIHYkAMiAExIAZ2Ng7WIQbWNQ7c+RgAADD4GAAAkBgAAACRGAAAAJEY\nAAAAkRgAAACRGAAAhP2tRQAAIABJREFUAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEY\nAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAA\nAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACR\nGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgA\nAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAA\nkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEY\nAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAA\nAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACR\nGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgA\nAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAA\nkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEY\nAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAA\nAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACR\nGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgA\nAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAA\nkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEY\nAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAA\nAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACR\nGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgA\nAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAA\nkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEY\nAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAA\nAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAACRGAAAAJEYAAAAkRgAAAA5DBKD1todW2vva61d01p7\namv/f3vnHmxXUe/5b19AgfBIQjCEPEgIISHhESAiPniJKAaDyB1HQAEHlGJGKJjyxb2jMjNCXZ0q\nnam6YCmWXLh6C0YkXDLKQwzhKWIikJAAITyEBPIATeQpgvb8sfdefvvrWX32OWc/1t75fqooep1e\ne61e3b/uXiu/b/86fDeE8NV63tEhhHXdLqNpL7YBYxswtgFjG9j6cJsPnW27XYAO8CUAS2KMcwc7\nMYTwWwCfiTH+olU3DyHcAeBwAG/V//RcjHFmq65vmqKrNlC/7ikALgYwBcAGAJ+OMd7dynuYLN0e\nB16RP+0A4DsxxvNbdQ8zKN22gakAvgPg3QDeAPATABfGGN/K/My0lm7bwH4ALgdwKIAXAHwxxnhD\nq65vBqTbbX4egE8DOADANTHGT0v+sajZxBQA96P2bvBMq+4/HPreYwBgLwCr2n2TUKOsPs+LMe5U\n/88fBZ2nqzYQQjgOwDcB/CcAOwM4EsBT7S6PSeiqDVD/3wnAHgBeB3Bdu8tjEro9F3wHwCYAEwDM\nBXAUgP/S7vKYhK7ZQAhhWwA3AvgpgLEAzgHwoxDCvu0uz1ZOt/v98wAuAXDlAL8ZB2AhgK+iZhPL\nAPzfdpazGfr6wyCEcDuAYwBcFkJ4JYSwbwjhqhDCJQOc+0PUvtj+X/3cL9X/fngI4ZchhC0hhOUh\nhKPpN3eEEC4NIdwL4DUAe3fkwUzTVMQG/geA/xlj/FWM8S8xxudijM+14XHNAFTEBpi/R+0F0R6j\nDlERG5gG4Mcxxj/GGDcAuAXAnJY/rBmQCtjALAB7AvjfMcY/xxhvB3AvgNPb8bymEm2OGOPCGOO/\nA/jdAEU8GcCqGON1McY/AvjvAA4KIcwa8cOPgL7+MIgxvh+1ybfxL/aPZ849HcCzABbUz/1fIYSJ\nAH6G2tfeWABfAHB9CGF3+unpqH357wygzP3zTyGEF0MI97JRmfbTbRsIIWwDYB6A3UMIT4QQ1oUQ\nLgsh7NDCxzQZum0DA3AmgH+NMcZhP5QZEhWxgf8D4JQQwo71630YtY8D0wEqYgNKALD/sB7IDEpF\n25yZA2A5leFVAE+iy/9g0NcfBi3gUwBuijHeVP+X3ttQc/XMp3OuijGuijG+FWN8c4BrfBm1r8iJ\nAK5A7Wt0ettLblrFSG1gPIDtAPwHAEegJiE4GMBXOlB20xpaMQ4AAEIIe6EmIbm6vUU2LaYVNnAX\nahP+SwDW1X//7+0uuGkZI7WB1ah5Cr8YQtguhPBB1MaCHTtSejMcWjb2l7ATgD/I3/6A2kdG1/CH\nQZ69AHy87kLaEkLYAuB9qGlEG6zNXSDGeH+M8eUY4xsxxqtRcx3Oz/3GVIqR2sDr9f//c4xxfYzx\nRQDfhm2glxjxOECcDuCeGOPTrS6kaSsjsoFQ0x7fgpqeeBSAcQDGoLb2yPQGI7KB+kvjSQBOQC0A\nxecB/Bi1j0RTTVo59g/EKwB2kb/tAuDlEVxzxGwNUYmGgrr21wL4YYzxs0P4TTP3CEP8jekcLbWB\nGOPmUAuHFps531SCdo4DZwD4xrBKZTpJq21gLGr65ctijG8AeCOE8C+oSRS+NKKSmnbR8nEgxrgC\nNS8BACCE8EvYe1glOvEOyKxCTVoKAAghjAIwHR1YLJ3DHoOUjUgXj/wIwIIQwodCCNuEELYPtbi3\nk5q5WAhhdP2324cQtg0hfBK1iDTWlVaXltpAnX8BcH4I4R0hhDEA/itqkSlMNWmHDSCE8B7UJIWO\nRlR9WmoDdU/h0wD+c30uGI3aC8GKlpfctIqWjwMhhAPrv9sxhPAF1P7l+arWFtuMgHa0+bYhhO0B\nbAOgcY3GP8rfAGD/EMLf18/5GoAVMcbHWvQ8w8IfBin/BOArdZfRF2KMawF8FMA/ohZzeC2AL6L5\netsOtX8RegHAiwDOB3BSbgGM6TqttgEA+DqApQAeB/AogAcBXNrSUptW0g4bAGovggtjjF11E5um\naIcNnAzg+PrvnwDwJmr/SGCqSTts4HQA61Fba3AsgOPqHiRTDdrR5l9BTVJ8EWprFl6v/w0xxhdQ\ni1J3KYDNAN4F4JTWPMrwCQ6MYYwxxhhjjLHHwBhjjDHGGOMPA2OMMcYYY8wIPwxCCMeHEFbXN266\nqFWFMr2DbcDYBgxgOzC2AWMb6AeGvcagvqPr4wCOQy0O71IAp8YYH2ld8UyVsQ0Y24ABbAfGNmBs\nA/3CSDwGhwF4Isb4VIzxTwCuRW31ttl6sA0Y24ABbAfGNmBsA33BSDY4m4h0x7d1qIVaKmWHHXaI\nu+yim7yZTvDSSy/h9ddfb/XGakO2gVGjRsUxY8aM6Kbq5Qph5I/ViuhczZZD71V279z1hvPMmzdv\nxquvvtp1G9hxxx3j6NGjW1yM6qFtxO2cy2snW7ZswWuvvdaODRaHZAejRo3aKmygimzZsqUd4wAw\nRBvYWsaBqrJ+/foXY4y7t/iyQ7KBnXbaKY4dO7bFRRiY3Jy5tUbnXLt27YA20Padj0MI5wA4BwB2\n3nlnnHLK4CFah9uAud/93d/91Tnyl7/8pelrNnuvVr9YttpQr7322pZebyiwDYwePRqf+9znRnS9\n4X4YsA0ob775Zun1y+6l19tmm21K89jm+F56zL/LXX84HwaXX375kH/TKtgGdt11V3z2s7mNJAcn\nZwPN9p1mxwsgbb/c9XNtlPsw+POf/zzgvfS8kX4Ef//73x/R70eC2sC55547ousNdy7I2cpwbGe4\nbZK7dyv+saOM7373u2279mCwDeyyyy44++yzy84rvcZw2mi4c0arfldGK951hvuucMkllzwzrB+O\nELaBMWPG4Etf6szG37n5X98JtxbOP//8AW1gJB8GzwGYTMeT6n9LiDFeAeAKABg/fvywLJgb9K23\n3irNy8ETr9Js52zFBKDl5eOylwO9dzsnjSEyZBuYNGnSiL949PnLbCDX5prHbaQDbdn1cy93mzdv\nTvLYbt94I93PZtSoUUWaPWr8kjkYXRzYhmwDe+65Z6kN5Noh155l/ehtb3tbch7X6Z/+9KfS6yn6\nMVdWxtyHI7e7Pgu333bbbTdgefV+uXt3YYwY1A7YBiZOnNjyf6Ire2at61wbcd1r/Zb1sVwb6W/K\n7FTz+Jo65+XaNjcOdMAmhmQDuXEgR+45ysZMrcPcvK7nMttu+9dXppwd8TWGUu9lL/m5j8hm34Fy\n128hQ7KBKVOmtLxAZXWfm0+1L/K52qd4HC/7Rz29Bo8reu5Q2q9TjKRESwHMCCFMCyG8DbXd2ha1\nplimR7ANGNuAAWwHxjZgbAN9wbA9BjHGt0II5wG4FcA2AK6MMa5qWclM5bENGNuAAWwHxjZgbAP9\nwojWGMQYbwJwU4vKYnoQ24CxDRjAdmBsA8Y20A+0ffFxGarbYo1XTnOleawTZm3Z9ttvn5zH2sAy\nvfBAeaxR4zKqbnCHHXYovSY/K5dD75ergwqtK6gcXL+s69O65vr94x//mOTttttupXnNan+ffvrp\nAcsEAHPmzBnwegrbg16D75dbQN/LtpLTwJa1g+Yxqu187bXXirSuMWDtaG7xMaP9nn/3+uuvJ3l8\nP73e29/+9iLN9qdrJHLaVH7W3HqMXqZZO8+t52D0GlyHWr9sO2x/O+20U3Iet5mOJWX6ZCCds7jM\n2ifYVpTcGpQqUtaGzS68bdYG9Ho8N+g4nlv8z3mczq1jUfvgc3WMYHIBU3LrkHKBEnp5bmC4zV55\n5ZUkj5+Z1+xpf+a+qfXCY/WmTZuSvFdffbVI77rrrkX6He94R3Jebr7KrUOqAtVb9WCMMcYYY4zp\nOP4wMMYYY4wxxnRPSqSwe01dQzvuuGORVrcLu2PV7V52noaLZDePXuPll18u0ipLYNgtpe47vqa6\nLdktxeep26tfpCLDpVnXWy5UJde9uvamT59epB999NEkr6xt169fn5w3ZcqUIs3SJACYNm1akV67\ndm2St2HDhgHvpbbC7m+1RX6eXnIfNysdAvJ7BJTtIaGSQs7TcYZlQVquF198ccB7qxxk3bp1peXn\na2q5eBzg8YlD2QLpWJhD7btfpUUM9wmWaGgd5shJfbjdWc6jdcvH2odzMlYe11i2pNIhHiNyc1Iu\nVGrVGK7sKRdSlvNy4an1fYDJycR4/NC6ZomJlpHbNjfG8Xk6lzU7H/aCnKwMrptmJV1A2t95HuYx\nFkjneZWVcj+dMGFCknfssccWae7fais8r6tkrMpzMmCPgTHGGGOMMQb+MDDGGGOMMcbAHwbGGGOM\nMcYYVGiNAWtuVTfJOn/Vk7HekvW3qhtkDVluDYBqc7ds2VKkd9555wF/A+T1cKx5U01hWUgr1aLm\nNHVV1o62itzW5mV1rzbAGv1cO//ud79L8lg7yCFJR48enZy3YMGCIr1w4cIBnqIG69C1zNy2bG9A\n8yF9q65fzJHTzubC/pb1YdXkT5o0acDz9Bqq6y7TK6sN3HXXXUX6+eefLy3/mDFjkjy21YcffrhI\njx07Njlvjz32KNKsYwb+9nl6haHoh/ncXP9gTe/UqVOT87gtVfvLum4eE4B0juJ215C1/Luctljn\nkM2bNxdpngNVG815ubDfOo9WeVzIrRkaSjhHnue5T2nf4P6tfTgXypTbiO1o3LhxyXmcp2uZuF10\nnmf74/ceLSPbTu5dQcfJKtuAws+h8x33MbVzHjN5vNRwotw3tQ9zeHFeOwik7cnz/MSJE5Pz+D1C\ny5hbG1QF+v+N0hhjjDHGGDMo/jAwxhhjjDHGdE9KlAvlp65fdslzCCggdbOOHz++SKt7bebMmUU6\nt3PkihUrkmMOd8WuIQ03yMd6fXZH8k58QFoPZbs4A+W7+26N5ORY3O45yZHKjF566aUirVKz5557\nrkjvtddeRfrUU09Nzlu+fHmR1pCn7NLk0JdAKlVil3Fu52a1j9wui1WkLIxebjdRPtbwcmW7Qqsb\nPycpZGmKhrhk2c7kyZOL9J577pmcd8YZZxRplYps3LixSLMkAQD233//Is2u8G9+85vJeX/4wx+K\ntNpALjRhFcMWNso0FHlDzrZXrlxZpLkt99lnn+S8O++8s0izXANIx3Htf5zH4zjLP4B0TtJ65/Lz\nmKN5PCbkQnRr3fXaONAgZwO5Xchzu5ezjJDfDYC0DnV3ap7nNY/bhdMqDXnhhReKtPZTLrO20e67\n716k+d1DxxK2MZWr8blqO71kE0zOzjWPx1m2AR5jAeDCCy8s0mpHjz/+eJH+5S9/meTxGMz2ptJD\ntjGdT3guy7WJ2h+Tq4ORysvtMTDGGGOMMcb4w8AYY4wxxhjjDwNjjDHGGGMMurDGoExPxRo91nED\nwIEHHlikL7jggiSPtZ6sp5s9e3ZyHusGVU92zDHHFGnV8s2bN69IX3vttUVadeKsGdMQiXy/3/72\ntyiD1yao9pX1jKp97Udymmhtv7Jwn7pFee4aHD5y2bJlSR7ryL/85S8X6euuuy45b9GiRUX6qKOO\nSvLYJg466KAkj/vEE088UaRVH8qaU9Us5tZIDPb3bpILU6jlZd2k1g23Z64ucut4WLfLYWmBVDPM\nda/roTjM3bRp05I81qhrKNN77rmnSJ9wwglFWscS1rPrs3GdaL32UphCbmddy1UW1lV/d+SRRxZp\n1fI/++yzRZo13UCq6WUtMQBMmDBhwHLomM7aYtX6cjuoLp3tKrdOhtH5NBfKtHHvKo4DCs+nWt7c\nWiueQ3nc1pCvvG5M24jnV51D+JjnaF1nwnakcw2vIdIxgs9l+9Prs/1p387ZQC+R08nzMz711FNJ\nHrftrbfeWqR1vt53332LtK5J5brXMKdsAzwn63sZl1HzuN/mQiZzHei7qR4zuj5vqNhjYIwxxhhj\njPGHgTHGGGOMMaYLUqKGa0vdgxymcenSpUneJZdcUqQPO+ywJI/DSrEbTt0z69evL9J77713kse7\n3v3gBz9I8th9zCEGc7tdqtuS3U2cBlJXKLuNVEqUC1vFLqVekgzkGEp4Mq4rdqGp+5XRcJG8G7G6\nX0877bQizaHLLr300tLzDj300CSPw+CyLAVIXY7sZlY5HEsWVIbAz6ryi4bbshdsg12u6irlZ1TX\nLEtuOLSohink81TOwzZwwAEHJHncp9l1/atf/So5j3c+1l2L2V2t8pCyHbtVKsLjptYBX1/rrhfa\nvgGXXcMKcxhglXl86EMfKtJcTxo6mOtCx9nf//73RZpDXANpCNTHHntswPLqsfZTHndyYa35vJwc\nROdR7vtarsYY2gu2kAs7zfWWGwfYdlQayGOEysnYJvTeXC6WMaucjOd5tVMeW3SOevLJJwe8t/YD\nRm0gF0a1V6VFKiti2V2u/VgSxBIjIK1rDicOpLsY5+ScPI5r3bJ9aCjTnNyV30+5LfUdkH+ncrWR\nYo+BMcYYY4wxxh8GxhhjjDHGmC5IiRruD3WfsMv/mmuuSfK++tWvFumTTjopybv77ruLNLt1Vq9e\nnZzHLlZ1D/Kx7kzMrhx2L2lUInYV6fX53upuYhcZ14HKQdgN1qu7Fw6F3DOqa4/PZXebune57seN\nG5fksWtSI0VwW1988cVFWuUmX/va14q02vAjjzxSpDVKynnnnVekWcqwZs2a5Dx+HrUPdiWqDVcx\nCklDzpDb1VTd7M1GnOJ+pLI+loBoRBPu3yo35HZhe+CdjoF0V2QdI1jSkou8xvJFHcdykVByu96O\nNEpFJ2GZJkfpAtJ+evLJJyd53PdZuqcueI4ywtHqgFQ+pH2MJWTcx/Qa3G9nzJiR5PHYpRIIhiUg\nKkMok04CqXxIy19lG8iNUfocOakZj4Ms+2GpMpC2A/dtIB2fNeoYw9HlpkyZkuRxuTSP5YYqb+G2\nZTtVCTIfqxyOpVa5iFZVh8d7HavZllUGxPXNbcvyMSCVGfFuyUBah2p/ZfJvla2yHemcz1EQDz/8\n8CSP25bnq02bNiXn5WSlI41caY+BMcYYY4wxxh8GxhhjjDHGGH8YGGOMMcYYY9CFNQYNbbGGTGOt\nnYYR5DBxuR1fOdyg6rFYr6ehTFnTy6FRgXR3vOnTp5eeV7bzKpDqyfhZgFQ3zs+9zz77JOexZkx1\n2VXWjubI6Upzu12q5o81uKwnVp04axG1znKhMHk9wjnnnFOk1cY4JK6GSORyHXzwwUkeayJ5d0bt\nBxwCVfsP27Tm9ZJ9sC5YnyO3doA1vdx+GqaQ+yb3bSBtZ12fwrpx1h3z2gAgDYGn5WdNOe9uDKQ6\ndQ6BqhphfjZdr6ShCZkqh6jUcrMN6DoNDkmqIaMffPDBIs39TdejPPPMMwPeCwDWrl1bpLXO+Nxc\nn2L70JC1fKz6dZ4bWAOv1+DxTkMfs6ZatfNVHge0HRid8zmUq64f4bmc5wkd03m8eOCBB5I8Xmuk\nIV9//etfF2m2I74vAEyaNKlIqwae+/5NN92U5PG8l9u5mdcQqf6+2byqkysrP5fWDfcPflfS8zjU\nve6iznOtrmPhcZfnbm3n/fffv0irffD7Br+3AsB9991XpLl/6ziZC9tbVl4gv6N0cc5gJ4QQrgwh\nbAohrKS/jQ0h3BZCWFP//5jcNUxvYxswgO3A2AaMbcDYBvqdZqREVwE4Xv52EYDFMcYZABbXj03/\nchVsA8Z2YGwDxjZgbAN9zaBSohjjXSGEqfLnjwI4up6+GsAdAL7czA0bbgx1az700ENFWsM3Mbff\nfntyzG4YDiHH7kAgdasuWrQoyeMwdHPmzEnyOMQVh0o9+uijk/NYSvTss88meSwTUCkR349dYBqS\nk12aKlXiutTdNFvhOmy1DZTBrnu1D3aHqcyD3XIs09EwfxwyVOVIjMo39ttvvyLNLsFbb701OY/b\nUl3cbH+HHHJIknfDDTcUabYjlbqwJEJdxFzmMsnASMOWtsMO9DnYzckSvPr9i7SGiOQwoVzXWhfs\n4md5IZCG/dOdsVnOwfag4TR5HNO+zmOJ9lPe7Z1tU8Msct/PhXrV526VlKiVNtAov8o1WP6lbcTy\nG6177jtsVzoesw28+93vTvJ03mBYusSyM7VFDqGtbnz+nUoIWM7A9qfSQx4H1E75OCfPGQmtsoEY\nY+lOvCwf0nCcHGZSn5Fth+UVakfvfOc7i7SGnWY54IYNG5I87n8s31y2bFlyHvdvfr8AgNNOO61I\nn3XWWUkezxs8lqislOf1nFSkXdKhTrwT5OYrfi49ryycrfZT7uuHHXZYkveud72rSOtczvKenFSJ\npWYqieRr6jstz2Vcfn0nZPvWdubf6TzREilRCeNjjA1B9QYA43Mnm77ENmAA24GxDRjbgLEN9A0j\njkoUa59rpZ92IYRzQgjLQgjLdNGv6Q+GYgP6VW36h5wdsA3oxlymf2jWBjwO9C8eB0yzNtDLG7D1\nM8P9MNgYQpgAAPX/byo7McZ4RYxxXoxxnkaRMD3NsGxAI0yYnqcpO2AbUJeo6XmGbAMeB/oOjwNm\nyDaQ28HedI/hhitdBOBMAN+o///GoV5AdYOsezrqqKOSvPvvv79Ir169uvR3HC5Sr88fJaq/Za34\nzTffnOSxxpdD2en1Wdesei/WSqvWlwdHDkelZWRNmmrq+Fh/14yebJiM2AZy6HOwllT/pYn1otOm\nTSvSCxcuTM7jbc85/CSQrkdgOwJS++B1LLNmzUrO43/9YJsF0tCpGor2lltuKdK8hkG10WzDWges\naS0LUdemkJXDsoOGzWqZuA9oHutsdUJhvSW3n57HGm8NN8s6UNUu8zjAWna1U14XomuBZs+eXaRX\nrVqV5D322GMDllHD0PG6BbZFINXW5sL9toERjQXqSeaxjkNE67kc4hpI+zA/v/Y3DjGo+uHFixeX\nXp/Har7Gsccem5x30EEHFWldA8Dzl9oHh1vlUIp33HFHch63O4e0BNLxQ+2j0bdGutaohCHbQAih\n6OO6zoTtVb1LvN5H51puTx6PdT0KzwUaNpaPdY0Pr1XgNtK1CDwGXXbZZUne5ZdfXqR1DuEwmfze\nkFsvou8i3L76O63nFjOicSCnhdd1FPxcul6QbYfrgsNdA2l4UZ1r2Ob0+mwT3O4aOpjfPzR0MI/x\nagPcRjzP6zV4rtxtt92SvNz6g2b6fzPhSq8BcB+AmSGEdSGEs1Fr+ONCCGsAfKB+bPoU24ABbAfG\nNmBsA8Y20O80E5Xo1JKsY0v+bvoM24ABbAfGNmBsA8Y20O90fOfj4sbiIme3jrpA77333iKtLiUO\nQcXuIHUfsxu11QIVAAAcXklEQVR47ty5SR7LMNTNwu6bY445pkjrbom8060+G7ubWI6k5efwWSqB\n4FCHuVCe6p7uJbjuVQ7D7jXeURBId59dsmRJkebwpEDqStQwhfPmzSvSvLslkIZHZbdtTnKkuyV+\n/etfL9IrV65M8rj97rrrriKtLm4Ok6khW3OLORv20iYJwYhQNyf3YX0mtgkNBcqw5IplP3oNlWNx\nO6hrmduTXcbaznxN1dFzuVQiw1IzdqHz2KfXV5lAzn1cxV1vG/aoCxBzO8Vz3auEgtuWx0sNU8jh\nLjVsLIeW1N1QObwoj7O6do7rniUDQDoGaWhUlijMnz+/SOd27VVpCJelrG9VaRfsRll0bOK+qHMh\n9w99fq4rloBouEiW7vGYC6SSRQ5bCaSyj5xcbcaMGUVa7e83v/lNkVaJE9sV30vDrbLd6loNbvd2\nhS3uBLnwyyyj0f7H/ZttRW2MpWC6qzC/O+g7Fe9Mz9JGHY9Y1sY7tgPp3K7zRFm5dGdlHi9U8sZz\nw3ACPbRNgG6MMcYYY4zpHfxhYIwxxhhjjPGHgTHGGGOMMaaLawxU28moVov1UnvttVeSx7oxDiGn\n+krWq6mmi9c0aLlYb8g6RdYEA8CCBQuKNIed03vvvvvuSR7rRXkLbdUgcx2o5o21lLoGo3FcRX15\nDtX8sdZYw4dxWMG77767SH/kIx9JzuNQplqH3C5qYwznqTb6iCOOKNIf/OAHkzw+97rrrkvy2N5Z\nm8o2BaT2p2swOE/rR+uyCjS0rrkwdKqN5OdS7TnXIevw1e65P2v4N763rh1gzTqvGdJwl7l1CrkQ\njKyV5vFIx6rcOMB6WtWlV5EyvTOPwaoh1zZjuM3YPtQGWBes64k4vKjWPa/x4bUOagMcOlDDiU6e\nPLlI50J05mL85+qH6aUxX8vKdq71y2gISobr/rjjjkvyWJeuff3GG/8aZZPHCyBdH8BjOq+LAYBD\nDjmkSHM4bQD4xCc+UaR1/GP7e/rpp4u0hvTl+tLxnt99emEcaAZ9n+M+oOvIGK4nXRPIY4mGwef1\nRGofbHNsR+973/uS8/h9QMcSXiM4c+bMJI/XHnH4a11Lwe89vGZGURtrJoS9PQbGGGOMMcYYfxgY\nY4wxxhhjuiAlarhL1Z3B7iAONwWk4cpUQsHHvJOphjzlXTHV/crXV5cgywY4JJTuSsvXVBc5hyjV\nsFtcTr63lpHrR6VWuXJpmLNuE2MsdXGz21Nd6ez2U9cY1wdLvE488cTkPA5BdueddyZ5XN8cshBI\ndzZlt5+GrGWpEocuBVI71d1Q2QY+9rGPFWkNc8dhTtW1zP1JpUNt3vm4pXDbqvuYQz9yfwbSnYTZ\nvjSMG+8My+0FpBIQDQfLY0szISGBv7VTHgeWLl2a5LG9s0tar8HyiJzEQsfJKkoK9NkacF/Uc9it\nr3IsrsNc+GuW6OXqRduIxyCWEqmkkOcvLSOP4zoH8jyUk9KwdETbmetLx4Eq9/+cvEGfg59ZxwHu\npxwKVENEs9TsU5/6VJJ37rnnFmltPx5neHxWqQiXWcvI8iR91+FnY6mgSidZSpMLrawyoyozFFkp\nj8E5ORbLSnNh3pWDDz64SH/84x9P8njM57bUkKTc1zUsLb+XsewMSPsCj0EcEh/I9wOe93SOspTI\nGGOMMcYY0xT+MDDGGGOMMcZ0VkoUYyzcN7moM7r6OrfbJ8sG2JWsrj2W6fDOckDqhlEpDksP2H3H\nEiMgXT2urhsuo+ZxtBOWSvB9gfTZ1CXGq+TVrd1wyzfjPuoUZZGS2LWnkQbYda8RoVj2wb9TN+qP\nfvSjIq3twPXL0iEgtSWWLanci1246v5nF7dKwc4444wizfa9fPny5DyWD+luoGybVdzlVmkmYoqO\nEdyeuvs1R/FiV7LKSLjNtB1YfnLUUUcleXwdlpSwtEDLvHHjxiRvzZo1RVojWLC9cHQ1latplBtG\nbZqpmoyEJYUq5+E6VMkcH6vcsExqxvUOpHWqUpyTTz65SGv7PfnkkwOWQ8eqnAyIny0n8+DxOrez\nrVLFCGTNoOMBH+s4y5I/nQv4XI7q84tf/CI5jyU8Os7OmzevSB9//PFJHt+Py6ERb9j+VDrK44c+\nW1lUM43GxfdW+XOzUXqqDpdV5zt+bxo7dmySx7KdXAQvnjPOOuusJI/ncu1TPE/Mnj27SOtO6fwu\nqWXkueeKK65I8jjy4f3331+k+ZmB1CY02iXPV/rOnJNQNajO26IxxhhjjDGma/jDwBhjjDHGGOMP\nA2OMMcYYY0yH1xiEEEq17qz1zOnhVU/H+kvWhKouk8NMql6N1zDo2gHW8vH1WTMOAMccc0yR5vUG\nQKoJVb0X6195V8UDDzwwOY93zNSwVXxNDokI/FUfr/ftJo32VS08t4Pu6sp6S31G/h3XE4cnBYB9\n9tmnSGv7sUZP17jwGo5DDz20SOuOlhzmVMOhzp07t0h/8pOfTPJmzZpVpK+88soirdpG1jVr+Lqy\ndRBVpWzn49w6E97pUfM4/BtrSXXnUrYd7etsj1OnTk3yWNvP+k21U657tb8PfOADRVrbj/s026Y+\nJ5dRNeqcV8XwpEwIobABHe9ZW6y7jjK6jozDR3Jd3Hfffcl5rNtVzfVtt91WpHV9Co8fvAZMz9PQ\nigxrjbVtefdyrhPVFufWOTG6rqRq60yY3HozHc94HZ2uqeO5l9cK8LodAPjpT39apJcsWZLk3X77\n7UX66quvTvLmz59fpDnMpM6vHC4yN59oHrcnv6foWMXr6lhHD6TtrGtcqmwDOXSM4HcqnvOBtF34\neXNr73huAdLxmNc3Aum8zPOOvpfw2hJ+t9Nz9Z2W3x24X7z3ve9NzuPxSNcwMNq3dF3VQNhjYIwx\nxhhjjPGHgTHGGGOMMaaL4UpzsHwHSCU8uistu97WrVtXpNVdwqGknnnmmSRvxowZRVpd/OyyYhe/\nurjZLaxuHb4Gh7wDUvc3uw7V7cV1ohICLovu9Frl0JUado+fS12g/BzqVnzPe95TpNmly7IfADjy\nyCOL9M9+9rMk7+c//3mRvvnmm5M8tj92A6utsKxEd9W94IILivSDDz6Y5C1cuLBIc4gzDU3J7nAN\nZ8uhy7TNc+ENq0bZTuZA6k7XcYRtJ7cLOR9riEEOb8hSAyDtVxMnTizSKtnhPqwhSVkeeMMNNyR5\nHN6Q212lBiydULkk5+lzV3n3a+3POVc3n6tjMI8Z3C48vgP53eBZrqfjbFndaXm5HTSMIMs+VCLD\nz8Ntq9fn/qx1wDKHXg1dCqR1rdIsHgd0rOPxk8dLlfdyX9T6ZRmJvit873vfK9JsixwqV++tYzXP\nGyprZnvhOURlcywv092TefxT+6hi/2+gYzq3u4Z85fctDT/PEhvuR3p9Hi/0vYzriWVbQPoOyudx\n6FIgDWuq7cw2oZJCLhdL4PV9hud8rZ/cGOpwpcYYY4wxxpim8IeBMcYYY4wxxh8GxhhjjDHGmA6v\nMWBU98whm1S3u2rVqiKtYaVY78s6PNX8cWhJDgsHpNpw1YSyHovDVmmIKdamqiaS733NNdckeVxm\n1hSqtpif88QTT0zyWE+mmsvG9uC5sHadJBemkNtd9b0c1lW3Hv/JT35SpFlTyNp9INUa6zX4+qz3\nBtK25nUs++23X3LehRdeOOBvgFR3fPHFFyd5RxxxRJGeNGlSkeYwpkC6dkVDqLHuVm2nijRCqKkN\ncNlVC85jhmoqeW1JWcg/RW2MQ8pqHh+zbpftAUi1xe9///uTvMWLFxdpHi+AdBxgzanWD9um9nUe\nq8pCgGroum7SGAe0TLkyarswrN9nW9E1X7l1V6zv1TpkzT5fX9cK8DimGm8+1t/xNfneqglmXbOG\nquy1dQVlbc11oeFguX5VX8968MbcBwArVqxIzuP6Vd09j7satpjnWh5ndF0k25E+I4fX5nkHSOcJ\nHrt0zVrZmiognQPVhqvyHtAM3C4aDpbXftxyyy1JHtsE9zHti2xXe++9d5LH6xR0fQevVczBa0nU\nPtiGda0Y34/fI3g9KpDOPbomk++tdZfbDqA4Z9AzjDHGGGOMMX2PPwyMMcYYY4wxnd/5uOEiZHca\nkLrGNDwUu4B0B0MO28VuPnXPsDv5zDPPTPI47JO6FcvcNSztAVJph4bP+vCHP1ykdUdjdnWxHEQl\nClzGnFtRJVoN2VQzu911G657DQXKEi+VArC8jJ9T7YjdtOpWPP3004u0hjfktuC2PeGEE5LzuB10\nt1W+N4cxA4ADDjigSLMbUcN18jHbA5B3jVeRRhnVXvk51OXJfV0lf+yqZTmS1iHL/ObMmZPksQxD\npVpsc2WhRYG0f6tkkdtM7VvdxA1yu95qO7N7usphipWh2GtOZlQWllelOHye3lvnDYbtkceZnHxH\n5zmWraqLn4/ZVlT+weOMShRyEqQqycgGg9tI5YB8rP2IpRe5OY/rgkNQ6/XVPnh85hCXKnfi62u9\ncxuphIVDJnP59RpcPyp1yYWnrrIN6HjP72z6jDz+T58+vfSauXGQ6yIX/lplwTzOcpn1XtyfNSQp\nz1FqwzyecEhtfSfksUrlw2wDuXDHZQzqMQghTA4hLAkhPBJCWBVCuKD+97EhhNtCCGvq/x8z2LVM\nb2IbMLYBYxswtgFjG+h/mpESvQXg8zHG2QAOB/C5EMJsABcBWBxjnAFgcf3Y9Ce2AWMbMLYBYxsw\ntoE+Z9APgxjj+hjjA/X0ywAeBTARwEcBXF0/7WoAJ7WrkKa72AaMbcDYBoxtwNgG+p8hrTEIIUwF\ncDCA+wGMjzE2BFAbAIwv+dmAqKaLj1UvxZo/Dv0JAEuWLCnSrCfeuHFjch7r9zkUFZBqsLRcrCdm\nbZnqSlkLpqGjWA+nIQxZW7x8+fIiraHsWK+sz8b1pWswGmVpZhvsZmilDShcRtXV8roK1RTyVuQc\nqkzDxrKmkENTAsDo0aOL9MqVK5O8Rx99dMBy6Fb3P/zhD4s0b2UOpFpVXZ/CWkpuWw63B6S6x5yu\nVMP9tlpX2gobaGgzVcPLbaY2y3pqDdPI4T95fYD2I+77uk6B21nXGHD9sq0cdNBByXm8rkW10dwu\nGhKX7Z3rQMOaso5V+wjXV07X3AraOQ6MoExN/T23piGnGeb65bbUtQL8O12zwOuL1D7YvtluVRPM\n6xTUPvi43XryTtlALpythq/l+uY5mtd4Aen4wf0Z+Nt5g+Frch/WOZnbQccgtgGdr3m9A7+X6JjO\nNqz2zPWT+10raOc7Idu9vm9xm2n78XW4b+p7GdeF3pvHAbW/sjFYr8G2qDp/vobaJpeL7Urbkp8t\n1676u2ZoerYIIewE4HoAF8YYX+K8WKu5AUehEMI5IYRlIYRlOoiZ3qIVNpAbdE31aYUN6IJM01t4\nHDC2AdMKG9CXYlMNmvowCCFsh5oB/FuMsbFr1MYQwoR6/gQAmwb6bYzxihjjvBjjvF7YeMkMTKts\nQP+l2/QOrbKBXOQXU208DhjbgGmVDeQ2oDTdY1AfQ6j5KH4A4NEY47cpaxGAMwF8o/7/G5u5YcPd\nou4Ndq1oCDaWb0yePDnJYwkPSzv0XyNWr15dpDkkGJC6FTWUJLuw2Igfe+yx5Dx2FalUiUPP3XPP\nPUleWXjGww47LDmPw5qxLApIn1XDGzbcziPZ8bDVNtAgJ2/S8F7qri+DPz7VBc8h5dTG+F8uli5d\nmuTxLtqHH354kVapD8tPVIrC5VfbZ/cx74ytu26yPEmlNM8++2yRbkeoynbZgHoR2SZ052N26apr\nlq/DUgvd9ZbDi/LumUAqQ9OXFm4jTutO7CoRKiujth+XM+dC57bNSUVaLR0C2mcD3UTrie1P3fPc\nFrlQh9y2GsaS7VbHcZbC8r11TOe83O7MOg60QlrUKRvgZ9Ry556fj3NSC64nHi/0GjqH8LzMsmb1\ngvL4oWGLv/WtbxXp3K7FjI5j/DxaRpYutUNO1ikbYPvV94HcHFdmH/oRkuvrufcl7tM57zdfX8Oj\n8++ef/75JI/bjMul8yHL6PSdIhe6uhlZeTPio/cCOB3AwyGEh+p/+0fUGv/HIYSzATwD4D82cS3T\nm9gGjG3A2AaMbcDYBvqcQT8MYoz3AChb2XBsa4tjqohtwNgGjG3A2AaMbaD/6ejOxzHGwk2iUgCO\nzqM7zbErTt1+7DJhF/++++6bnJfb1ZRdK+qaZIlJLioRu3w0Ws2KFSuKtLoVeUU9u3x0J1R2T6sM\ngc9Vt1HDrdgLOx8z2g451xi7/bhd1L3GLleNZjFv3rwirbbDkWbYpTl//vzkvKOPPrpIaxtx9Cx1\nHbLdfuYznynS6qZ84IEHBvwNkNqV7qQ4nKgE7abRhurq5rJrG7HUQuuXn5Hd8doXc9FC+JrqtmUJ\nI8uHWMoIpFIltdPcDrlcrpwLmml1hJGtEW0jPlaJRrMyLm4/na9Y9qn2x1FT+DztB7ldU9mGdQxt\nZsfTqpCr39xcwMe5qD4sM1JpRU7uxeMsSz1VpsJy4uOOOy7JmzRpUpHWeZ7fN3hsydmRlp9lTCrB\nbVV0wnagbcRl1XKX7T4MpO3H/UGvweOnzie5cnE/LbvXYOXPSQW5/VjGrPA8pxGX2D703a8ZmWnr\nhajGGGOMMcaYnsMfBsYYY4wxxhh/GBhjjDHGGGM6vMYAKNcWs9Ze9VKs61a9FGt/WZOtYQM53Jfq\nMjdv3lykc6HRcppF1ghrGZ966qkireHrWD/Kv9N1EKw11lBarD/sF91xTlOodc/1xhpN1W9ySM+H\nHnooyWNNudbhnXfeWaRZF8xrCgBg1qxZRXrGjBlJHusIOSwmANx9991FmsOo8toGINXYa/i6XOiy\ndoSuHCmNfqZ1zWXN6evVPlhbzHpffXbu66q/5X6lYQNZW8ztomFNWdup+mc+N7fmJxeal/XlOlbl\nftcv40IryOnXWYefqzPO0/GY213XCXEbqeaf2za3509OX92PaDs0q93mPqbvFDxP6LsCj/Ea2pzz\nyrTmQBpefMGCBUne3LlzUQa/6/C9NFwn205u3YCOQVUbB0IIhQ1r2bhOtY+xnl/HPV2TU3Ye951c\nOFtt27Id0HW+4jbS3Zn5ebRtuVxl9wJS+8g9W1t3PjbGGGOMMcb0L/4wMMYYY4wxxnRWShRCKNwa\n6hriYw3pydIAlYew655lArqj6sSJEwe8nt4v55plF5W6f3IhBjn86vTp05M8dgGxa0jdm1xGdQ2x\nDEYlJg13Z6+7nJsND5hrB3YXqpyHWbduXXLMbcE2cP311yfnsWuZ7U3vrW5tbjMOlaphe6dMmVKk\n1UXMbsteaOtGfagtczurezQXppCfn8/TuuZjvXcuzB+PM+wWVskH/y4XnjQX7phtWOUmOu6UXcMM\nj2Z33M3BbaaSNJWxMmzTObkMk+vrvWwPubLndqzl45z8lN839H2A3x1Y2gmk4wBLOzkNpCHKVbbK\ncmIObwwABxxwwIDnaRnLxgsgnaOGsqNvt8nt1K2hQDlPx1LuE7kdkhmtQ353yIX75LSWg3+n/ZQl\nhjpH8RzC7a5jSdl4MVheLjRrUd5BzzDGGGOMMcb0Pf4wMMYYY4wxxvjDwBhjjDHGGNOFcKVl2kHW\nfOs5ua2f99hjjwHPU43w2rVri7TqdlnLpxpQ1mexri0XUlVDjfI1tVz8rHyehrdiNAQel1HLpc/a\nq+R0tqybZL2+6kNnzpxZpFU3yBrUQw89tPT63F4PP/xwch6vA9F2YM2fboHO5eQ81QLysa5ByWlO\nq4zquHMhSrmPqV2zvjMXyo+PVffJ2mLN42vm1g6wraiNsQ3kNNQ53XhurQ3Ty/rybsL1q3XNx8Ot\nX7aBXFvy9ftlDB8Kzdp57ne5fpTThvMYPGHChNJrcFvqnM/rHXU9G68l1DG+2XGcy5wLtVl1YozF\nc+rz8nMM1x54DNbxuCy0LZC2rdpRWVl0HQuXX6/B5+bmKLYVXt8CpOOC2lHZOohmscfAGGOMMcYY\n4w8DY4wxxhhjTBekRGXk3C5lO9sCqfyGQzvldqtTuUKZDEGP2b2koaNyO5LmdkjkcuVcgHzN3K6Y\nSrPhunqZsmfMue9yuwHmwoKxPejuxhx6Tu/Ntqmu3zIby5Ux534sc3UO1x3bTnLhBrW8XB/aT/l3\nXIe53YC1fnOSvDK3fi6UXW5X51xbWC5UDYZbvzkbNu2F6z7XfjlZWE5uWDbXjBs3LjlmSetQxuqy\nMULLkRtnmn3uqpGTvOR2ite5oCxUZ+76eo3cWF1Wv7l3u5w0VUPIlu1srraX2xm6bPfkgY4Hwh4D\nY4wxxhhjjD8MjDHGGGOMMf4wMMYYY4wxxqCLawxyui3Nazb8Il8jt/V3Lq9ZTf5QQkBx+au8JXm/\nMRRtHduc6tJZD5jTrPPaj6GEoywLkahrYZoNW1h27ypp0odTlmbD8OXC0DE53W7u3OGGrWxW/5wj\nF06z7F6ms1RZx91vNGvnzerE9Vx9Hyjr+7nr59YtajlaYTtbm/3l3tlyWvtmx/t2ULYOInfvnB0p\nIw1Zbo+BMcYYY4wxxh8GxhhjjDHGGCB00u0UQngBwDMAxgF4cZDTO8HWVI69Yoy7t/keg2IbKMU2\n0D22pnLYBgZmayqHbWBgtrZydN0ObAOldNUGOvphUNw0hGUxxnkdv7HLURmq8swuR/eoyjO7HN2j\nKs/scnSPqjyzy9E9qvLMLkcNS4mMMcYYY4wx/jAwxhhjjDHGdO/D4Iou3VdxObpHVZ7Z5egeVXlm\nl6N7VOWZXY7uUZVndjm6R1We2eVAl9YYGGOMMcYYY6qFpUTGGGOMMcaYzn4YhBCODyGsDiE8EUK4\nqIP3vTKEsCmEsJL+NjaEcFsIYU39/2M6UI7JIYQlIYRHQgirQggXdKss3cI2YBvolg3U7911O7AN\n2AZsA7YB20ANvxNUzw469mEQQtgGwOUAPgxgNoBTQwizO3T7qwAcL3+7CMDiGOMMAIvrx+3mLQCf\njzHOBnA4gM/V66AbZek4tgEAtoFu2gBQDTuwDdgGbAO2ga3aBoCu28FV6L4NAFW0gxhjR/4D8G4A\nt9LxPwD4hw7efyqAlXS8GsCEenoCgNWdKguV4UYAx1WhLLYB28DWYANVtAPbgG3ANmAb2NpsoAp2\nUDUbqIoddFJKNBHAWjpeV/9btxgfY1xfT28AML6TNw8hTAVwMID7u12WDmIbIGwDALpvA0AX6942\nAMA2MBW2AdvA1mcDQPXswO8E8OJjAECsfZJ1LDxTCGEnANcDuDDG+FI3y2Jq2AYM0Nm6tw1UE9uA\nsQ2YrfmdoJMfBs8BmEzHk+p/6xYbQwgTAKD+/02duGkIYTvUGv/fYowLu1mWLmAbgG0A1bIBoAt1\nbxuwDdgGbANbuQ0A1bMDvxOgsx8GSwHMCCFMCyG8DcApABZ18P7KIgBn1tNnoqbraishhADgBwAe\njTF+u5tl6RK2AdtA1WwA6HDd2wZsA7YB24BtAED17MDvBEDnFh/XF1DMB/A4gCcB/LcO3vcaAOsB\nvImahu1sALuhttJ7DYBfABjbgXK8DzV30AoAD9X/m9+NsnTrP9uAbaBbNlAVO7AN2AZsA7YB20B3\n7aAKNlBVO/DOx8YYY4wxxhgvPjbGGGOMMcb4w8AYY4wxxhgDfxgYY4wxxhhj4A8DY4wxxhhjDPxh\nYIwxxhhjjIE/DIwxxhhjjDHwh4ExxhhjjDEG/jAwxhhjjDHGAPj/LkVbu/fivDEAAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<Figure size 864x864 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YdTzsKJTguq",
        "colab_type": "text"
      },
      "source": [
        "**SECOND CODE**\n",
        "\n",
        "This is my second code. The basic architecture is defined in the first layer. The goal is to achieve\n",
        "\n",
        "99.4% validation accuracy using less than 15K parameters.\n",
        "The concepts applied here are as following :\n",
        "\n",
        "> Large Batch Size\n",
        "\n",
        "The First Model acheived validation accuracy of 99.01% with 14,776 parameters.\n",
        "\n",
        "The First Model with larger batch size (96) achieved 98.92% accuracy with 14,776 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpX5bw1XED1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0414feca-0e5a-480f-d20d-1231a16b6434"
      },
      "source": [
        "from keras.layers import Activation\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model2.add(Convolution2D(10, 3, 3,activation='relu')) #24\n",
        "model2.add(Convolution2D(20, 3, 3,activation='relu')) #22\n",
        "model2.add(Convolution2D(10, 3, 3,activation='relu')) #20\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2))) #10\n",
        "\n",
        "model2.add(Convolution2D(16, 3, 3,activation='relu')) #8\n",
        "\n",
        "model2.add(Convolution2D(10, 3, 3,activation='relu')) #6\n",
        "model2.add(Convolution2D(20, 3, 3,activation='relu')) #4\n",
        "\n",
        "model2.add(Convolution2D(10, 4,4)) #1\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train, Y_train, batch_size=96, nb_epoch=10, verbose=1)\n",
        "\n",
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_319 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_320 (Conv2D)          (None, 24, 24, 10)        2890      \n",
            "_________________________________________________________________\n",
            "conv2d_321 (Conv2D)          (None, 22, 22, 20)        1820      \n",
            "_________________________________________________________________\n",
            "conv2d_322 (Conv2D)          (None, 20, 20, 10)        1810      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_323 (Conv2D)          (None, 8, 8, 16)          1456      \n",
            "_________________________________________________________________\n",
            "conv2d_324 (Conv2D)          (None, 6, 6, 10)          1450      \n",
            "_________________________________________________________________\n",
            "conv2d_325 (Conv2D)          (None, 4, 4, 20)          1820      \n",
            "_________________________________________________________________\n",
            "conv2d_326 (Conv2D)          (None, 1, 1, 10)          3210      \n",
            "_________________________________________________________________\n",
            "flatten_45 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,776\n",
            "Trainable params: 14,776\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.3600 - acc: 0.8831\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1022 - acc: 0.9679\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0706 - acc: 0.9779\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0552 - acc: 0.9826\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0492 - acc: 0.9847\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0431 - acc: 0.9866\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0406 - acc: 0.9870\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0359 - acc: 0.9879\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0335 - acc: 0.9888\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0304 - acc: 0.9902\n",
            "[0.037423631040076726, 0.9882]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EJ2ILZmFmhr",
        "colab_type": "text"
      },
      "source": [
        "**THIRD CODE**\n",
        "\n",
        "This is my code. The basic architecture is defined in the first layer. The goal is to achieve\n",
        "\n",
        "99.4% validation accuracy using less than 15K parameters.\n",
        "The concepts applied here are as following :\n",
        "\n",
        "> Batch Normalization\n",
        "\n",
        "The First Model acheived validation accuracy of 99.01% with 14,776 parameters.\n",
        "\n",
        "The Second Model (First Model with larger batch size (96)) achieved 98.92% accuracy with 14,776 parameters.\n",
        "\n",
        "The Third Model (Second Model with larger batch normalization) achieved 98.98% accuracy with 14,960 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcUqi0IzFlh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "013738b6-6eb6-4b18-bfca-379d399dd3f0"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model3.add(Convolution2D(10, 3, 3,activation='relu')) #24\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "model3.add(Convolution2D(20, 3, 3,activation='relu')) #22\n",
        "model3.add(Convolution2D(10, 3, 3,activation='relu')) #20\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2))) #10\n",
        "\n",
        "model3.add(Convolution2D(16, 3, 3,activation='relu')) #8\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "model3.add(Convolution2D(10, 3, 3,activation='relu')) #6\n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "model3.add(Convolution2D(20, 3, 3,activation='relu')) #4\n",
        "\n",
        "model3.add(Convolution2D(10, 4,4)) #1\n",
        "\n",
        "model3.add(Flatten())\n",
        "model3.add(Activation('softmax'))\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model3.fit(X_train, Y_train, batch_size=96, nb_epoch=10, verbose=1)\n",
        "\n",
        "score = model3.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_329 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_330 (Conv2D)          (None, 24, 24, 10)        2890      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 24, 24, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_331 (Conv2D)          (None, 22, 22, 20)        1820      \n",
            "_________________________________________________________________\n",
            "conv2d_332 (Conv2D)          (None, 20, 20, 10)        1810      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 20, 20, 10)        40        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_49 (MaxPooling (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_333 (Conv2D)          (None, 8, 8, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_334 (Conv2D)          (None, 6, 6, 10)          1450      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 10)          40        \n",
            "_________________________________________________________________\n",
            "conv2d_335 (Conv2D)          (None, 4, 4, 20)          1820      \n",
            "_________________________________________________________________\n",
            "conv2d_336 (Conv2D)          (None, 1, 1, 10)          3210      \n",
            "_________________________________________________________________\n",
            "flatten_46 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,960\n",
            "Trainable params: 14,868\n",
            "Non-trainable params: 92\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 12s 194us/step - loss: 0.2285 - acc: 0.9325\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0591 - acc: 0.9814\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0450 - acc: 0.9861\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0372 - acc: 0.9881\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0319 - acc: 0.9900\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 108us/step - loss: 0.0284 - acc: 0.9911\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0243 - acc: 0.9919\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0218 - acc: 0.9932\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0203 - acc: 0.9932\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.0193 - acc: 0.9936\n",
            "[0.03595059181679353, 0.9898]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85KbDfcOHEl-",
        "colab_type": "text"
      },
      "source": [
        "**FOURTH CODE**\n",
        "\n",
        "This is my fourth code. The basic architecture is defined in the first layer. The goal is to achieve\n",
        "\n",
        "99.4% validation accuracy using less than 15K parameters.\n",
        "The concepts applied here are as following :\n",
        "\n",
        "> Dropout\n",
        "\n",
        "The First Model acheived validation accuracy of 99.01% with 14,776 parameters.\n",
        "\n",
        "The Second Model (First Model with larger batch size (96)) achieved 98.92% accuracy with 14,776 parameters.\n",
        "\n",
        "The Third Model (Second Model with larger batch normalization) achieved 98.98% accuracy with 14,868 parameters.\n",
        "\n",
        "The Fourth Model (Third Model with larger dropout) achieved 99.14% accuracy with 14,868 parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDFxlWQ5HPEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "678ce442-fe5f-404b-d4f8-7f98baf5eb94"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "\n",
        "model4 = Sequential()\n",
        "model4.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model4.add(Convolution2D(10, 3, 3,activation='relu')) #24\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.25))\n",
        "\n",
        "model4.add(Convolution2D(20, 3, 3,activation='relu')) #22\n",
        "model4.add(Convolution2D(10, 3, 3,activation='relu')) #20\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.25))\n",
        "\n",
        "model4.add(MaxPooling2D(pool_size=(2, 2))) #10\n",
        "\n",
        "model4.add(Convolution2D(16, 3, 3,activation='relu')) #8\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.25))\n",
        "\n",
        "model4.add(Convolution2D(10, 3, 3,activation='relu')) #6\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.25))\n",
        "\n",
        "model4.add(Convolution2D(20, 3, 3,activation='relu')) #4\n",
        "\n",
        "model4.add(Convolution2D(10, 4,4)) #1\n",
        "\n",
        "model4.add(Flatten())\n",
        "model4.add(Activation('softmax'))\n",
        "\n",
        "model4.summary()\n",
        "\n",
        "model4.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model4.fit(X_train, Y_train, batch_size=96, nb_epoch=10, verbose=1)\n",
        "\n",
        "score = model4.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_50\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_353 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_354 (Conv2D)          (None, 24, 24, 10)        2890      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 24, 24, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_355 (Conv2D)          (None, 22, 22, 20)        1820      \n",
            "_________________________________________________________________\n",
            "conv2d_356 (Conv2D)          (None, 20, 20, 10)        1810      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 20, 20, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 20, 20, 10)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_52 (MaxPooling (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_357 (Conv2D)          (None, 8, 8, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_358 (Conv2D)          (None, 6, 6, 10)          1450      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 6, 6, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_359 (Conv2D)          (None, 4, 4, 20)          1820      \n",
            "_________________________________________________________________\n",
            "conv2d_360 (Conv2D)          (None, 1, 1, 10)          3210      \n",
            "_________________________________________________________________\n",
            "flatten_49 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,960\n",
            "Trainable params: 14,868\n",
            "Non-trainable params: 92\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 13s 212us/step - loss: 0.3566 - acc: 0.8854\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0979 - acc: 0.9695\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0720 - acc: 0.9773\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0593 - acc: 0.9815\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0558 - acc: 0.9831\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0512 - acc: 0.9837\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0464 - acc: 0.9857\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0416 - acc: 0.9869\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.0422 - acc: 0.9866\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0399 - acc: 0.9878\n",
            "[0.026690236092568376, 0.9914]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onmQiAplID-N",
        "colab_type": "text"
      },
      "source": [
        "**FIFTH CODE**\n",
        "\n",
        "This is my fifth code. The basic architecture is defined in the first layer. The goal is to achieve\n",
        "\n",
        "99.4% validation accuracy using less than 15K parameters. The concepts applied here are as following :\n",
        "\n",
        "> Learning Rate Scheduler\n",
        "\n",
        "The First Model acheived validation accuracy of 99.01% with 14,776 parameters.\n",
        "\n",
        "The Second Model (First Model with larger batch size (96)) achieved 98.92% accuracy with 14,776 parameters.\n",
        "\n",
        "The Third Model (Second Model with larger batch normalization) achieved 98.98% accuracy with 14,868 parameters.\n",
        "\n",
        "The Fourth Model (Third Model with larger dropout) achieved 99.1% accuracy with 14,868 parameters.\n",
        "\n",
        "The Fifth Model (Fourth Model with learning rate scheduler) achieved 99.04% accuracy with 14,868 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN6mCddZIEfL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba13e4e6-d2c8-46e5-f625-a3a66e7ab777"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "model5 = Sequential()\n",
        "model5.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model5.add(Convolution2D(10, 3, 3,activation='relu')) #24\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.25))\n",
        "\n",
        "model5.add(Convolution2D(20, 3, 3,activation='relu')) #22\n",
        "model5.add(Convolution2D(10, 3, 3,activation='relu')) #20\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.25))\n",
        "\n",
        "model5.add(MaxPooling2D(pool_size=(2, 2))) #10\n",
        "\n",
        "model5.add(Convolution2D(16, 3, 3,activation='relu')) #8\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.25))\n",
        "\n",
        "model5.add(Convolution2D(10, 3, 3,activation='relu')) #6\n",
        "model5.add(BatchNormalization())\n",
        "model5.add(Dropout(0.25))\n",
        "\n",
        "model5.add(Convolution2D(20, 3, 3,activation='relu')) #4\n",
        "\n",
        "model5.add(Convolution2D(10, 4,4)) #1\n",
        "\n",
        "model5.add(Flatten())\n",
        "model5.add(Activation('softmax'))\n",
        "\n",
        "model5.summary()\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model5.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model5.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "score5 = model5.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score5)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_61\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_441 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_442 (Conv2D)          (None, 24, 24, 10)        2890      \n",
            "_________________________________________________________________\n",
            "batch_normalization_57 (Batc (None, 24, 24, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_53 (Dropout)         (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_443 (Conv2D)          (None, 22, 22, 20)        1820      \n",
            "_________________________________________________________________\n",
            "conv2d_444 (Conv2D)          (None, 20, 20, 10)        1810      \n",
            "_________________________________________________________________\n",
            "batch_normalization_58 (Batc (None, 20, 20, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_54 (Dropout)         (None, 20, 20, 10)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_63 (MaxPooling (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_445 (Conv2D)          (None, 8, 8, 16)          1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_59 (Batc (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_55 (Dropout)         (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_446 (Conv2D)          (None, 6, 6, 10)          1450      \n",
            "_________________________________________________________________\n",
            "batch_normalization_60 (Batc (None, 6, 6, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_56 (Dropout)         (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_447 (Conv2D)          (None, 4, 4, 20)          1820      \n",
            "_________________________________________________________________\n",
            "conv2d_448 (Conv2D)          (None, 1, 1, 10)          3210      \n",
            "_________________________________________________________________\n",
            "flatten_60 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_60 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,960\n",
            "Trainable params: 14,868\n",
            "Non-trainable params: 92\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.2395 - acc: 0.9228 - val_loss: 0.1050 - val_acc: 0.9691\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0728 - acc: 0.9776 - val_loss: 0.0415 - val_acc: 0.9848\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 101us/step - loss: 0.0564 - acc: 0.9824 - val_loss: 0.0339 - val_acc: 0.9895\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0500 - acc: 0.9841 - val_loss: 0.0315 - val_acc: 0.9900\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0452 - acc: 0.9861 - val_loss: 0.0282 - val_acc: 0.9900\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0410 - acc: 0.9873 - val_loss: 0.0372 - val_acc: 0.9885\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 103us/step - loss: 0.0365 - acc: 0.9884 - val_loss: 0.0275 - val_acc: 0.9905\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.0355 - acc: 0.9884 - val_loss: 0.0287 - val_acc: 0.9912\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0340 - acc: 0.9888 - val_loss: 0.0299 - val_acc: 0.9897\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0315 - acc: 0.9896 - val_loss: 0.0268 - val_acc: 0.9911\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0303 - acc: 0.9905 - val_loss: 0.0314 - val_acc: 0.9897\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0286 - acc: 0.9909 - val_loss: 0.0287 - val_acc: 0.9906\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0288 - acc: 0.9910 - val_loss: 0.0278 - val_acc: 0.9915\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0264 - acc: 0.9912 - val_loss: 0.0250 - val_acc: 0.9918\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0262 - acc: 0.9913 - val_loss: 0.0251 - val_acc: 0.9919\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0255 - acc: 0.9921 - val_loss: 0.0199 - val_acc: 0.9929\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.0245 - acc: 0.9925 - val_loss: 0.0263 - val_acc: 0.9911\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 6s 100us/step - loss: 0.0235 - acc: 0.9922 - val_loss: 0.0222 - val_acc: 0.9926\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0243 - acc: 0.9923 - val_loss: 0.0224 - val_acc: 0.9927\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.0226 - acc: 0.9928 - val_loss: 0.0304 - val_acc: 0.9904\n",
            "[0.030355823072404017, 0.9904]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ig7gOs7NS20",
        "colab_type": "text"
      },
      "source": [
        "**SIXTH CODE**\n",
        "\n",
        "Now we have all parameters added to our model. Let's try to achieve same accuracy with lesser number of parameters.\n",
        "\n",
        "The model achieved 98.78% accuracy with 7250 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKFU-2QwNnY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "127a4002-699d-4150-f8bd-01387a893592"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "model6 = Sequential()\n",
        "model6.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model6.add(Convolution2D(10, 3, 3,activation='relu')) #24\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.25))\n",
        "\n",
        "model6.add(Convolution2D(10, 3, 3,activation='relu')) #22\n",
        "model6.add(Convolution2D(10, 3, 3,activation='relu')) #20\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.25))\n",
        "\n",
        "model6.add(MaxPooling2D(pool_size=(2, 2))) #10\n",
        "\n",
        "model6.add(Convolution2D(10, 3, 3,activation='relu')) #8\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.25))\n",
        "\n",
        "model6.add(Convolution2D(10, 3, 3,activation='relu')) #6\n",
        "model6.add(BatchNormalization())\n",
        "model6.add(Dropout(0.25))\n",
        "\n",
        "model6.add(Convolution2D(10, 3, 3,activation='relu')) #4\n",
        "\n",
        "model6.add(Convolution2D(10, 4,4)) #1\n",
        "\n",
        "model6.add(Flatten())\n",
        "model6.add(Activation('softmax'))\n",
        "\n",
        "model6.summary()\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model6.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model6.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "score6 = model6.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score6)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_62\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_449 (Conv2D)          (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "conv2d_450 (Conv2D)          (None, 24, 24, 10)        910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_61 (Batc (None, 24, 24, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_57 (Dropout)         (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_451 (Conv2D)          (None, 22, 22, 10)        910       \n",
            "_________________________________________________________________\n",
            "conv2d_452 (Conv2D)          (None, 20, 20, 10)        910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 20, 20, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_58 (Dropout)         (None, 20, 20, 10)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_64 (MaxPooling (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_453 (Conv2D)          (None, 8, 8, 10)          910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 8, 8, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_59 (Dropout)         (None, 8, 8, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_454 (Conv2D)          (None, 6, 6, 10)          910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 6, 6, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_60 (Dropout)         (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_455 (Conv2D)          (None, 4, 4, 10)          910       \n",
            "_________________________________________________________________\n",
            "conv2d_456 (Conv2D)          (None, 1, 1, 10)          1610      \n",
            "_________________________________________________________________\n",
            "flatten_61 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_61 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 7,330\n",
            "Trainable params: 7,250\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.3556 - acc: 0.8850 - val_loss: 0.0828 - val_acc: 0.9742\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0973 - acc: 0.9701 - val_loss: 0.1074 - val_acc: 0.9660\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0780 - acc: 0.9755 - val_loss: 0.0511 - val_acc: 0.9840\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0682 - acc: 0.9794 - val_loss: 0.0559 - val_acc: 0.9826\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0636 - acc: 0.9805 - val_loss: 0.0474 - val_acc: 0.9847\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0577 - acc: 0.9815 - val_loss: 0.0495 - val_acc: 0.9848\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0547 - acc: 0.9828 - val_loss: 0.0456 - val_acc: 0.9857\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0518 - acc: 0.9842 - val_loss: 0.0449 - val_acc: 0.9862\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.0484 - acc: 0.9851 - val_loss: 0.0492 - val_acc: 0.9861\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0460 - acc: 0.9857 - val_loss: 0.0472 - val_acc: 0.9853\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0454 - acc: 0.9863 - val_loss: 0.0388 - val_acc: 0.9876\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0425 - acc: 0.9867 - val_loss: 0.0662 - val_acc: 0.9793\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0427 - acc: 0.9867 - val_loss: 0.0472 - val_acc: 0.9862\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0416 - acc: 0.9868 - val_loss: 0.0326 - val_acc: 0.9899\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.0392 - acc: 0.9878 - val_loss: 0.0428 - val_acc: 0.9860\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0389 - acc: 0.9878 - val_loss: 0.0364 - val_acc: 0.9886\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 6s 96us/step - loss: 0.0381 - acc: 0.9878 - val_loss: 0.0382 - val_acc: 0.9885\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0370 - acc: 0.9884 - val_loss: 0.0433 - val_acc: 0.9867\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0389 - acc: 0.9874 - val_loss: 0.0333 - val_acc: 0.9898\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0377 - acc: 0.9880 - val_loss: 0.0436 - val_acc: 0.9878\n",
            "[0.0436074388734065, 0.9878]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6kj7NVoRRP1",
        "colab_type": "text"
      },
      "source": [
        "**SEVENTH CODE**\n",
        "Let's use sixth model and change the optimizer to SGD.\n",
        "The accuracy is 92.29%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe1-6PwRQCar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8be83fd-1f0d-4d10-cc36-7fcb7da2b3a5"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras import optimizers\n",
        "\n",
        "model7 = Sequential()\n",
        "model7.add(Convolution2D(10, 3, 3, activation='relu', input_shape=(28,28,1))) #26\n",
        "model7.add(Convolution2D(10, 3, 3,activation='relu')) #24\n",
        "model7.add(BatchNormalization())\n",
        "model7.add(Dropout(0.25))\n",
        "\n",
        "model7.add(Convolution2D(10, 3, 3,activation='relu')) #22\n",
        "model7.add(Convolution2D(10, 3, 3,activation='relu')) #20\n",
        "model7.add(BatchNormalization())\n",
        "model7.add(Dropout(0.25))\n",
        "\n",
        "model7.add(MaxPooling2D(pool_size=(2, 2))) #10\n",
        "\n",
        "model7.add(Convolution2D(10, 3, 3,activation='relu')) #8\n",
        "model7.add(BatchNormalization())\n",
        "model7.add(Dropout(0.25))\n",
        "model7.add(Convolution2D(10, 3, 3,activation='relu')) #6\n",
        "model7.add(BatchNormalization())\n",
        "model7.add(Dropout(0.25))\n",
        "\n",
        "model7.add(Convolution2D(10, 3, 3,activation='relu')) #4\n",
        "\n",
        "model7.add(Convolution2D(10, 4,4)) #1\n",
        "\n",
        "model7.add(Flatten())\n",
        "model7.add(Activation('softmax'))\n",
        "\n",
        "model7.summary()\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "sgd = optimizers.SGD(lr=0.003)\n",
        "\n",
        "model7.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "model7.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "score7 = model7.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print(score7)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (4, 4))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_508 (Conv2D)          (None, 26, 26, 10)        100       \n",
            "_________________________________________________________________\n",
            "conv2d_509 (Conv2D)          (None, 24, 24, 10)        910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 24, 24, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_85 (Dropout)         (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_510 (Conv2D)          (None, 22, 22, 10)        910       \n",
            "_________________________________________________________________\n",
            "conv2d_511 (Conv2D)          (None, 20, 20, 10)        910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_90 (Batc (None, 20, 20, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_86 (Dropout)         (None, 20, 20, 10)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_71 (MaxPooling (None, 10, 10, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_512 (Conv2D)          (None, 8, 8, 10)          910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_91 (Batc (None, 8, 8, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_87 (Dropout)         (None, 8, 8, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_513 (Conv2D)          (None, 6, 6, 10)          910       \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 6, 6, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_88 (Dropout)         (None, 6, 6, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_514 (Conv2D)          (None, 4, 4, 10)          910       \n",
            "_________________________________________________________________\n",
            "conv2d_515 (Conv2D)          (None, 1, 1, 10)          1610      \n",
            "_________________________________________________________________\n",
            "flatten_68 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 7,330\n",
            "Trainable params: 7,250\n",
            "Non-trainable params: 80\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 20s 335us/step - loss: 1.7791 - acc: 0.3945 - val_loss: 1.0577 - val_acc: 0.6990\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.8649 - acc: 0.7386 - val_loss: 0.6582 - val_acc: 0.8119\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.6169 - acc: 0.8132 - val_loss: 0.5442 - val_acc: 0.8424\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.5140 - acc: 0.8444 - val_loss: 0.4640 - val_acc: 0.8672\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.4532 - acc: 0.8620 - val_loss: 0.4194 - val_acc: 0.8768\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.4161 - acc: 0.8748 - val_loss: 0.4010 - val_acc: 0.8827\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.3870 - acc: 0.8822 - val_loss: 0.3681 - val_acc: 0.8909\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.3686 - acc: 0.8878 - val_loss: 0.3556 - val_acc: 0.8967\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.3514 - acc: 0.8933 - val_loss: 0.3356 - val_acc: 0.9030\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.3392 - acc: 0.8978 - val_loss: 0.3265 - val_acc: 0.9048\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.3282 - acc: 0.8994 - val_loss: 0.3187 - val_acc: 0.9073\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.3176 - acc: 0.9023 - val_loss: 0.2976 - val_acc: 0.9123\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.3103 - acc: 0.9048 - val_loss: 0.2964 - val_acc: 0.9133\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.3029 - acc: 0.9072 - val_loss: 0.2889 - val_acc: 0.9142\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2938 - acc: 0.9096 - val_loss: 0.2816 - val_acc: 0.9167\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.2885 - acc: 0.9113 - val_loss: 0.2801 - val_acc: 0.9181\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2861 - acc: 0.9124 - val_loss: 0.2752 - val_acc: 0.9183\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2807 - acc: 0.9125 - val_loss: 0.2644 - val_acc: 0.9209\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2755 - acc: 0.9159 - val_loss: 0.2637 - val_acc: 0.9228\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2735 - acc: 0.9159 - val_loss: 0.2591 - val_acc: 0.9229\n",
            "[0.25912199917435647, 0.9229]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}